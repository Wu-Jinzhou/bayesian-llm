{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Verify environment, load Llama-3.1-8B (base), and run tokenization + probability-extraction sanity checks so later experiments arenâ€™t confounded by tokenizer quirks (e.g., `\"X\"` vs `\" X\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook path setup: make repo imports work regardless of where you run this from\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_candidates = [cwd, cwd.parent]\n",
    "repo_root = next((p for p in repo_candidates if (p / 'bayesian_llm').exists()), None)\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(f'Could not find repo root from cwd={cwd}.')\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print('Repo root:', repo_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check (no installs happen automatically)\n",
    "import os\n",
    "import pkgutil\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED = [\n",
    "    'torch',\n",
    "    'transformers',\n",
    "    'accelerate',\n",
    "    'huggingface_hub',\n",
    "    'transformer_lens',\n",
    "]\n",
    "\n",
    "missing = [p for p in REQUIRED if pkgutil.find_loader(p) is None]\n",
    "print('Missing packages:', missing if missing else 'None')\n",
    "\n",
    "if missing:\n",
    "    req = Path('requirements.txt')\n",
    "    if not req.exists():\n",
    "        req = Path('../requirements.txt')\n",
    "    print(f'Install with: pip install -r {req}')\n",
    "\n",
    "print('HF_TOKEN set:', bool(os.getenv('HF_TOKEN')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "MODEL_ID = 'meta-llama/Llama-3.1-8B'  # base model\n",
    "DTYPE = 'float16'  # change to 'bfloat16' if your GPU supports it\n",
    "DEVICE_MAP = 'auto'\n",
    "\n",
    "print('MODEL_ID:', MODEL_ID)\n",
    "print('DTYPE:', DTYPE)\n",
    "print('DEVICE_MAP:', DEVICE_MAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model + tokenizer (can take a while)\n",
    "import torch\n",
    "\n",
    "from bayesian_llm.llm import load_hf_causal_lm\n",
    "\n",
    "dtype = {\n",
    "    'float16': torch.float16,\n",
    "    'bfloat16': torch.bfloat16,\n",
    "    'float32': torch.float32,\n",
    "}[DTYPE]\n",
    "\n",
    "try:\n",
    "    loaded = load_hf_causal_lm(MODEL_ID, torch_dtype=dtype, device_map=DEVICE_MAP)\n",
    "    model, tokenizer = loaded.model, loaded.tokenizer\n",
    "    print('Loaded:', MODEL_ID)\n",
    "    print('Vocab size:', tokenizer.vocab_size)\n",
    "    print('Model dtype:', next(model.parameters()).dtype)\n",
    "    print('First param device:', next(model.parameters()).device)\n",
    "except Exception as e:\n",
    "    print('Failed to load model.')\n",
    "    print('Common fixes:')\n",
    "    print('- Ensure you have accepted the model license on HuggingFace')\n",
    "    print('- Set env var HF_TOKEN to a valid token')\n",
    "    print('- If on CPU/MPS, expect this to be slow and may OOM')\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization sanity check for candidate answers\n",
    "import pandas as pd\n",
    "\n",
    "CANDIDATES = ['X', ' X', '\\nX', 'Y', ' Y', '\\nY', 'H', ' H', 'T', ' T']\n",
    "rows = []\n",
    "for s in CANDIDATES:\n",
    "    ids = tokenizer.encode(s, add_special_tokens=False)\n",
    "    rows.append({\n",
    "        'string': repr(s),\n",
    "        'n_tokens': len(ids),\n",
    "        'token_ids': ids,\n",
    "        'decoded': tokenizer.decode(ids),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df)\n",
    "print('Single-token variants:', df[df.n_tokens == 1]['string'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next-token probability extraction sanity check\n",
    "import torch\n",
    "from bayesian_llm.llm import normalized_next_token_prob\n",
    "\n",
    "prompt_abstract = (\n",
    "    'Two random generators. Generator A: 50% X. Generator B: 75% X. '\n",
    "    'Sequence: X X Y X X. '\n",
    "    'Predict the next output (X or Y):'\n",
    ")\n",
    "\n",
    "p_x = normalized_next_token_prob(\n",
    "    model, tokenizer, prompt_abstract,\n",
    "    a_variants=[' X', 'X', '\\nX'],\n",
    "    b_variants=[' Y', 'Y', '\\nY'],\n",
    ")\n",
    "\n",
    "print('P(next is X | {X,Y}):', round(p_x, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground-truth Bayes for the canonical A=0.5 vs B=0.75 task\n",
    "from bayesian_llm.bayes import two_generator_posterior_predictive\n",
    "\n",
    "# Sequence: X X Y X X has n_X=4 out of 5\n",
    "true_p = two_generator_posterior_predictive(n_x=4, n_total=5)\n",
    "print('True Bayes P(next is X):', round(true_p, 4))\n",
    "print('LLM - Bayes error:', round(p_x - true_p, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal reproducibility footprint\n",
    "import platform\n",
    "import transformers\n",
    "\n",
    "print('Python:', platform.python_version())\n",
    "print('Torch:', torch.__version__)\n",
    "print('Transformers:', transformers.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA device:', torch.cuda.get_device_name(0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
