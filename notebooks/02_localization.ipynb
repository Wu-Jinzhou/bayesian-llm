{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization\n",
    "\n",
    "Localizing where Bayesian evidence integration appears inside Llama-3.1-8B.\n",
    "\n",
    "We do three complementary analyses:\n",
    "1. Layerwise logit lens: decode the model's implied `P(next=X)` from intermediate hidden states and compare to Bayes.\n",
    "2. Belief-state probing: train a linear probe to predict the true posterior from internal activations, test cross-template generalization.\n",
    "3. Head patching: identify sparse heads whose activations causally restore Bayes-like behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook path setup: make repo imports work regardless of where you run this from\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_candidates = [cwd, cwd.parent]\n",
    "repo_root = next((p for p in repo_candidates if (p / 'bayesian_llm').exists()), None)\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(f'Could not find repo root from cwd={cwd}.')\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print('Repo root:', repo_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency check\n",
    "import pkgutil\n",
    "\n",
    "REQUIRED = ['torch', 'transformers', 'accelerate', 'huggingface_hub', 'sklearn', 'numpy', 'pandas', 'matplotlib']\n",
    "missing = [p for p in REQUIRED if pkgutil.find_loader(p) is None]\n",
    "print('Missing core packages:', missing if missing else 'None')\n",
    "\n",
    "HAS_TLENS = pkgutil.find_loader('transformer_lens') is not None\n",
    "print('transformer_lens available:', HAS_TLENS)\n",
    "if missing or not HAS_TLENS:\n",
    "    print('Install with: pip install -r ../requirements.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import numpy as np\n",
    "\n",
    "MODEL_ID = 'meta-llama/Llama-3.1-8B'\n",
    "DTYPE = 'float16'  # or 'bfloat16'\n",
    "DEVICE_MAP = 'auto'\n",
    "\n",
    "# Evidence sweep params\n",
    "N_TOTAL = 10\n",
    "N_PERMUTATIONS = 5\n",
    "CONTROL_TEMPLATE = 'order_irrelevant'  # 'base' | 'independent' | 'order_irrelevant'\n",
    "\n",
    "# Probe dataset params\n",
    "N_TRAIN = 200\n",
    "N_TEST = 100\n",
    "LAYERS_STRIDE = 4  # probe every k layers to keep compute manageable\n",
    "\n",
    "RNG_SEED = 0\n",
    "\n",
    "print('MODEL_ID:', MODEL_ID)\n",
    "print('N_TOTAL:', N_TOTAL)\n",
    "print('CONTROL_TEMPLATE:', CONTROL_TEMPLATE)\n",
    "print('Probe: N_TRAIN/N_TEST:', N_TRAIN, N_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace model (used for hidden-states logit lens + probing)\n",
    "import torch\n",
    "from bayesian_llm.llm import load_hf_causal_lm\n",
    "\n",
    "dtype = {\n",
    "    'float16': torch.float16,\n",
    "    'bfloat16': torch.bfloat16,\n",
    "    'float32': torch.float32,\n",
    "}[DTYPE]\n",
    "\n",
    "loaded = load_hf_causal_lm(MODEL_ID, torch_dtype=dtype, device_map=DEVICE_MAP)\n",
    "model, tokenizer = loaded.model, loaded.tokenizer\n",
    "\n",
    "print('Loaded HF model:', MODEL_ID)\n",
    "print('dtype:', next(model.parameters()).dtype)\n",
    "print('device:', next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt + Bayes helpers\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from bayesian_llm.bayes import discrete_posterior_predictive, DiscreteHypothesis, two_generator_posterior_predictive\n",
    "from bayesian_llm.data import make_sequence, permute_sequence, set_seed\n",
    "\n",
    "set_seed(RNG_SEED)\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "HYPOTHESES = [\n",
    "    DiscreteHypothesis(name='A', p_success=0.50),\n",
    "    DiscreteHypothesis(name='B', p_success=0.75),\n",
    "]\n",
    "\n",
    "X_VARIANTS = [' X', 'X', '\\nX']\n",
    "Y_VARIANTS = [' Y', 'Y', '\\nY']\n",
    "\n",
    "\n",
    "def token_ids_for_variants(variants):\n",
    "    ids = []\n",
    "    for v in variants:\n",
    "        enc = tokenizer.encode(v, add_special_tokens=False)\n",
    "        if len(enc) == 1:\n",
    "            ids.append(int(enc[0]))\n",
    "    ids = sorted(set(ids))\n",
    "    if not ids:\n",
    "        raise ValueError(f'No single-token ids for variants={variants}')\n",
    "    return ids\n",
    "\n",
    "X_IDS = token_ids_for_variants(X_VARIANTS)\n",
    "Y_IDS = token_ids_for_variants(Y_VARIANTS)\n",
    "print('X_IDS:', X_IDS, 'Y_IDS:', Y_IDS)\n",
    "\n",
    "\n",
    "def prompt_two_generators(sequence_tokens, *, control: str):\n",
    "    seq_str = ' '.join(sequence_tokens)\n",
    "    if control == 'base':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X.'\n",
    "    elif control == 'independent':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X. Draws are independent.'\n",
    "    elif control == 'order_irrelevant':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X. Draws are independent. Order does not matter.'\n",
    "    else:\n",
    "        raise ValueError(control)\n",
    "\n",
    "    return (\n",
    "        f\"{prefix} Sequence: {seq_str}. \"\n",
    "        'Predict the next output (X or Y):'\n",
    "    )\n",
    "\n",
    "\n",
    "def bayes_posterior_B(n_x, n_total):\n",
    "    _, post = discrete_posterior_predictive(\n",
    "        n_success=int(n_x),\n",
    "        n_total=int(n_total),\n",
    "        hypotheses=HYPOTHESES,\n",
    "        priors=[0.5, 0.5],\n",
    "    )\n",
    "    return float(post['B'])\n",
    "\n",
    "\n",
    "def logit(p):\n",
    "    p = min(1 - 1e-9, max(1e-9, float(p)))\n",
    "    return math.log(p / (1 - p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layerwise logit lens on HF hidden states\n",
    "# We decode each intermediate hidden state using the *final* norm + lm_head.\n",
    "\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def layerwise_p_x(prompt: str):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = inputs.to(next(model.parameters()).device)\n",
    "\n",
    "    out = model(**inputs, output_hidden_states=True, use_cache=False, return_dict=True)\n",
    "    h_states = out.hidden_states  # tuple: (embeds, layer1, ..., layerN)\n",
    "\n",
    "    # Llama uses a final RMSNorm before lm_head; apply it to intermediate states too.\n",
    "    norm = model.model.norm\n",
    "    lm_head = model.lm_head\n",
    "\n",
    "    probs = []\n",
    "    for h in h_states[1:]:  # skip embeddings\n",
    "        last = h[0, -1, :]\n",
    "        last = norm(last)\n",
    "        logits = lm_head(last)\n",
    "        p = torch.softmax(logits.float(), dim=-1)\n",
    "        p_x = float(p[X_IDS].sum().item())\n",
    "        p_y = float(p[Y_IDS].sum().item())\n",
    "        probs.append(p_x / (p_x + p_y) if (p_x + p_y) > 0 else 0.5)\n",
    "\n",
    "    return np.array(probs)\n",
    "\n",
    "# Smoke test\n",
    "prompt = prompt_two_generators(['X','X','Y','X','X'], control=CONTROL_TEMPLATE)\n",
    "px_by_layer = layerwise_p_x(prompt)\n",
    "print('n_layers decoded:', len(px_by_layer))\n",
    "print('first/last:', px_by_layer[0], px_by_layer[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evidence sweep: where (which layers) are most Bayes-aligned?\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "rows = []\n",
    "\n",
    "for n_x in tqdm(range(N_TOTAL + 1), desc='layerwise_sweep'):\n",
    "    base_seq = make_sequence(n_x=n_x, n_total=N_TOTAL, x='X', y='Y')\n",
    "    true_p_next = two_generator_posterior_predictive(n_x=n_x, n_total=N_TOTAL)\n",
    "\n",
    "    # Average over permutations to reduce order artifacts\n",
    "    px_layers = []\n",
    "    for _ in range(N_PERMUTATIONS):\n",
    "        seq = permute_sequence(base_seq, rng=rng)\n",
    "        prompt = prompt_two_generators(seq, control=CONTROL_TEMPLATE)\n",
    "        px_layers.append(layerwise_p_x(prompt))\n",
    "\n",
    "    px_layers = np.stack(px_layers, axis=0)  # [perm, layer]\n",
    "    mean_px = px_layers.mean(axis=0)\n",
    "\n",
    "    for layer_idx, px in enumerate(mean_px, start=1):\n",
    "        rows.append({\n",
    "            'n_X': n_x,\n",
    "            'layer': layer_idx,\n",
    "            'p_x_logit_lens': float(px),\n",
    "            'true_bayes_p_x': float(true_p_next),\n",
    "            'error': float(px - true_p_next),\n",
    "            'abs_error': float(abs(px - true_p_next)),\n",
    "        })\n",
    "\n",
    "df_layer = pd.DataFrame(rows)\n",
    "display(df_layer.head())\n",
    "\n",
    "# Aggregate per layer\n",
    "mae_by_layer = df_layer.groupby('layer')['abs_error'].mean().reset_index().sort_values('abs_error')\n",
    "display(mae_by_layer.head(10))\n",
    "\n",
    "best_layer = int(mae_by_layer.iloc[0]['layer'])\n",
    "print('Best layer by MAE:', best_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: MAE-by-layer and error heatmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.plot(mae_by_layer.layer, mae_by_layer.abs_error, marker='o')\n",
    "ax.set_title('Logit lens: MAE to Bayes vs layer')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Mean |P_x(layer)-P_x(Bayes)|')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap over (layer, n_X)\n",
    "pivot = df_layer.pivot_table(index='layer', columns='n_X', values='error', aggfunc='mean')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "im = ax.imshow(pivot.values, aspect='auto', origin='lower', cmap='RdBu_r', vmin=-0.25, vmax=0.25)\n",
    "ax.set_title('Logit lens error heatmap (blue = under, red = over vs Bayes)')\n",
    "ax.set_xlabel('n_X')\n",
    "ax.set_ylabel('Layer')\n",
    "ax.set_xticks(range(pivot.shape[1]))\n",
    "ax.set_xticklabels(pivot.columns.tolist())\n",
    "fig.colorbar(im, ax=ax, fraction=0.03, pad=0.04)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probe: is the Bayes posterior linearly represented? (predict P(B|evidence))\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Choose layers to probe (stride to keep compute manageable)\n",
    "n_layers = model.config.num_hidden_layers\n",
    "layers_to_probe = list(range(1, n_layers + 1, LAYERS_STRIDE))\n",
    "print('Layers to probe:', layers_to_probe)\n",
    "\n",
    "# Build dataset of prompts and targets\n",
    "\n",
    "def make_dataset(n_samples, control):\n",
    "    prompts, targets = [], []\n",
    "    for _ in range(n_samples):\n",
    "        n_x = int(rng.integers(0, N_TOTAL + 1))\n",
    "        seq = make_sequence(n_x=n_x, n_total=N_TOTAL, x='X', y='Y')\n",
    "        seq = permute_sequence(seq, rng=rng)\n",
    "        prompts.append(prompt_two_generators(seq, control=control))\n",
    "        targets.append(bayes_posterior_B(n_x=n_x, n_total=N_TOTAL))\n",
    "    return prompts, np.array(targets, dtype=np.float64)\n",
    "\n",
    "train_prompts, y_train = make_dataset(N_TRAIN, control='base')\n",
    "test_prompts, y_test = make_dataset(N_TEST, control=CONTROL_TEMPLATE)\n",
    "\n",
    "print('Train control:', 'base')\n",
    "print('Test control:', CONTROL_TEMPLATE)\n",
    "print('y_train range:', (y_train.min(), y_train.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract representations for selected layers (last-position hidden state)\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_layer_reprs(prompts, layers):\n",
    "    # returns dict layer -> [n, d_model]\n",
    "    feats = {layer: [] for layer in layers}\n",
    "    device = next(model.parameters()).device\n",
    "    norm = model.model.norm\n",
    "\n",
    "    for prompt in tqdm(prompts, desc='extract_reprs'):\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        out = model(**inputs, output_hidden_states=True, use_cache=False, return_dict=True)\n",
    "        h_states = out.hidden_states\n",
    "\n",
    "        for layer in layers:\n",
    "            h = h_states[layer]  # 1..n_layers inclusive (after that layer)\n",
    "            last = h[0, -1, :]\n",
    "            last = norm(last)\n",
    "            feats[layer].append(last.detach().float().cpu().numpy())\n",
    "\n",
    "    for layer in layers:\n",
    "        feats[layer] = np.stack(feats[layer], axis=0)\n",
    "    return feats\n",
    "\n",
    "X_train = extract_layer_reprs(train_prompts, layers_to_probe)\n",
    "X_test = extract_layer_reprs(test_prompts, layers_to_probe)\n",
    "\n",
    "print('Example layer feature shape:', layers_to_probe[0], X_train[layers_to_probe[0]].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit per-layer probes and evaluate cross-template generalization\n",
    "\n",
    "rows = []\n",
    "probes = {}\n",
    "\n",
    "for layer in layers_to_probe:\n",
    "    reg = Ridge(alpha=1.0, fit_intercept=True, random_state=0)\n",
    "    reg.fit(X_train[layer], y_train)\n",
    "\n",
    "    pred_train = reg.predict(X_train[layer])\n",
    "    pred_test = reg.predict(X_test[layer])\n",
    "\n",
    "    r2_tr = float(r2_score(y_train, pred_train))\n",
    "    r2_te = float(r2_score(y_test, pred_test))\n",
    "\n",
    "    rows.append({'layer': layer, 'r2_train': r2_tr, 'r2_test': r2_te})\n",
    "    probes[layer] = reg\n",
    "\n",
    "df_probe = pd.DataFrame(rows).sort_values('r2_test', ascending=False)\n",
    "display(df_probe)\n",
    "\n",
    "best_probe_layer = int(df_probe.iloc[0]['layer'])\n",
    "print('Best probe layer (by test R^2):', best_probe_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot probe quality by layer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.plot(df_probe.sort_values('layer').layer, df_probe.sort_values('layer').r2_test, marker='o', label='Test R^2')\n",
    "ax.plot(df_probe.sort_values('layer').layer, df_probe.sort_values('layer').r2_train, marker='x', label='Train R^2', alpha=0.7)\n",
    "ax.set_title('Linear probe for Bayes posterior P(B|evidence)')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('R^2')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter: probe prediction vs true posterior (on test prompts)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "layer = best_probe_layer\n",
    "reg = probes[layer]\n",
    "y_hat = reg.predict(X_test[layer])\n",
    "\n",
    "# Clip for visualization (regression can overshoot [0,1])\n",
    "y_hat_clip = np.clip(y_hat, 0, 1)\n",
    "\n",
    "mae_val = float(mean_absolute_error(y_test, y_hat_clip))\n",
    "print('Layer', layer, 'Test MAE:', mae_val)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(y_test, y_hat_clip, alpha=0.5)\n",
    "ax.plot([0,1],[0,1], color='black', linewidth=1)\n",
    "ax.set_title(f'Probe generalization (layer={layer})')\n",
    "ax.set_xlabel('True P(B|evidence)')\n",
    "ax.set_ylabel('Probe prediction')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Save probe for later causal steering (optional)\n",
    "import os\n",
    "os.makedirs('results', exist_ok=True)\n",
    "np.save(f'results/probe_w_layer{layer}.npy', reg.coef_.astype(np.float32))\n",
    "np.save(f'results/probe_b_layer{layer}.npy', np.array([reg.intercept_], dtype=np.float32))\n",
    "print('Saved probe weights to results/.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Patching (TransformerLens)\n",
    "\n",
    "Finds attention heads whose activations move the model from a corrupted condition toward a clean condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TransformerLens model (optional but recommended for head-level causal tracing)\n",
    "\n",
    "import pkgutil\n",
    "HAS_TLENS = pkgutil.find_loader('transformer_lens') is not None\n",
    "if not HAS_TLENS:\n",
    "    raise RuntimeError('transformer_lens not installed. Run: pip install -r ../requirements.txt')\n",
    "\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_act_name\n",
    "\n",
    "# Use CUDA if available; otherwise this will be extremely slow.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dtype = torch.float16 if DTYPE == 'float16' else torch.bfloat16\n",
    "\n",
    "tl_model = HookedTransformer.from_pretrained(MODEL_ID, device=device, dtype=dtype)\n",
    "print('Loaded TransformerLens model on', device)\n",
    "print('n_layers:', tl_model.cfg.n_layers, 'n_heads:', tl_model.cfg.n_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clean vs corrupt prompts (you can change these)\n",
    "\n",
    "clean_seq = ['X'] * 9 + ['Y'] * 1\n",
    "corrupt_seq = ['X'] * 5 + ['Y'] * 5\n",
    "\n",
    "clean_prompt = prompt_two_generators(clean_seq, control=CONTROL_TEMPLATE)\n",
    "corrupt_prompt = prompt_two_generators(corrupt_seq, control=CONTROL_TEMPLATE)\n",
    "\n",
    "print('CLEAN:', clean_prompt)\n",
    "print('CORRUPT:', corrupt_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline metrics (logit diff between X and Y)\n",
    "\n",
    "import torch\n",
    "\n",
    "X_tok = tl_model.to_single_token(' X')\n",
    "Y_tok = tl_model.to_single_token(' Y')\n",
    "\n",
    "clean_logits, clean_cache = tl_model.run_with_cache(clean_prompt)\n",
    "corrupt_logits, corrupt_cache = tl_model.run_with_cache(corrupt_prompt)\n",
    "\n",
    "def logit_diff(logits):\n",
    "    return float((logits[0, -1, X_tok] - logits[0, -1, Y_tok]).item())\n",
    "\n",
    "clean_ld = logit_diff(clean_logits)\n",
    "corrupt_ld = logit_diff(corrupt_logits)\n",
    "print('Clean logit_diff(X-Y):', clean_ld)\n",
    "print('Corrupt logit_diff(X-Y):', corrupt_ld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head patching sweep (partial by default)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Start small to ensure runtime is reasonable, then widen.\n",
    "LAYER_RANGE = range(max(0, tl_model.cfg.n_layers - 8), tl_model.cfg.n_layers)  # last 8 layers\n",
    "HEAD_RANGE = range(tl_model.cfg.n_heads)\n",
    "\n",
    "patching = np.zeros((tl_model.cfg.n_layers, tl_model.cfg.n_heads), dtype=np.float32)\n",
    "\n",
    "for layer in tqdm(LAYER_RANGE, desc='layers'):\n",
    "    hook_name = get_act_name('z', layer)\n",
    "\n",
    "    for head in HEAD_RANGE:\n",
    "        def patch_head(z, hook, head=head, hook_name=hook_name):\n",
    "            # z: [batch, pos, head, d_head]\n",
    "            z[:, :, head, :] = clean_cache[hook_name][:, :, head, :]\n",
    "            return z\n",
    "\n",
    "        patched_logits = tl_model.run_with_hooks(\n",
    "            corrupt_prompt,\n",
    "            fwd_hooks=[(hook_name, patch_head)],\n",
    "        )\n",
    "\n",
    "        patched_ld = logit_diff(patched_logits)\n",
    "        denom = (clean_ld - corrupt_ld)\n",
    "        recovery = (patched_ld - corrupt_ld) / denom if abs(denom) > 1e-8 else 0.0\n",
    "        patching[layer, head] = float(recovery)\n",
    "\n",
    "# Top heads\n",
    "flat = []\n",
    "for layer in LAYER_RANGE:\n",
    "    for head in HEAD_RANGE:\n",
    "        flat.append((patching[layer, head], layer, head))\n",
    "flat.sort(reverse=True, key=lambda x: x[0])\n",
    "print('Top 10 recovery heads:')\n",
    "for r,layer,head in flat[:10]:\n",
    "    print(f'  layer {layer:02d} head {head:02d}: recovery={r:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize patching heatmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "im = ax.imshow(patching, aspect='auto', origin='lower', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_title('Head patching recovery (clean -> corrupt)')\n",
    "ax.set_xlabel('Head')\n",
    "ax.set_ylabel('Layer')\n",
    "fig.colorbar(im, ax=ax, fraction=0.03, pad=0.04)\n",
    "plt.show()\n",
    "\n",
    "# Save for causal-intervention notebook\n",
    "import os\n",
    "os.makedirs('results', exist_ok=True)\n",
    "np.save('results/head_patching_recovery.npy', patching)\n",
    "print('Saved results/head_patching_recovery.npy')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
