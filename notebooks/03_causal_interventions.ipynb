{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Interventions\n",
    "\n",
    "Tests whether Bayes-like behavior comes from a specific internal inference mechanism by applying causal interventions:\n",
    "\n",
    "- Ablate candidate heads (identified in `02_localization.ipynb`) and measure changes in Bayes-alignment.\n",
    "- Steer / inject belief using a probe direction and see if downstream predictions shift predictably.\n",
    "- Stress-test robustness: distractors, symbol swaps, and order drift.\n",
    "\n",
    "If the model only matches Bayes behaviorally, these interventions should not produce clean, systematic effects. If there is an internal belief update mechanism, we expect targeted, interpretable changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook path setup: make repo imports work regardless of where you run this from\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_candidates = [cwd, cwd.parent]\n",
    "repo_root = next((p for p in repo_candidates if (p / 'bayesian_llm').exists()), None)\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(f'Could not find repo root from cwd={cwd}.')\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print('Repo root:', repo_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pkgutil\n",
    "missing = [p for p in ['torch','transformers','transformer_lens','numpy','pandas','matplotlib','tqdm'] if pkgutil.find_loader(p) is None]\n",
    "print('Missing:', missing if missing else 'None')\n",
    "if missing:\n",
    "    print('Install with: pip install -r ../requirements.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import numpy as np\n",
    "\n",
    "MODEL_ID = 'meta-llama/Llama-3.1-8B'\n",
    "DTYPE = 'float16'\n",
    "\n",
    "CONTROL_TEMPLATE = 'order_irrelevant'\n",
    "N_TOTAL = 10\n",
    "N_PERMUTATIONS = 5\n",
    "\n",
    "# Head selection from patching results\n",
    "TOPK_BAYES = 20\n",
    "TOPK_ANTI = 20\n",
    "\n",
    "# Steering\n",
    "STEER_LAYER = None  # if None, infer from saved probe filename\n",
    "STEER_ALPHA = 5.0   # scaling for probe direction; tune\n",
    "\n",
    "RNG_SEED = 0\n",
    "\n",
    "print('MODEL_ID:', MODEL_ID)\n",
    "print('CONTROL_TEMPLATE:', CONTROL_TEMPLATE)\n",
    "print('TOPK_BAYES/ANTI:', TOPK_BAYES, TOPK_ANTI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TransformerLens model\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.float16 if DTYPE == 'float16' else torch.bfloat16\n",
    "\n",
    "tl_model = HookedTransformer.from_pretrained(MODEL_ID, device=device, dtype=dtype)\n",
    "print('Loaded on', device)\n",
    "print('n_layers:', tl_model.cfg.n_layers, 'n_heads:', tl_model.cfg.n_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load localization artifacts (if present)\n",
    "import os\n",
    "import re\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "patch_path = 'results/head_patching_recovery.npy'\n",
    "probe_w_paths = sorted([p for p in os.listdir('results') if re.match(r'probe_w_layer\\d+\\.npy', p)])\n",
    "\n",
    "head_recovery = None\n",
    "if os.path.exists(patch_path):\n",
    "    head_recovery = np.load(patch_path)\n",
    "    print('Loaded', patch_path, 'shape=', head_recovery.shape)\n",
    "else:\n",
    "    print('No head patching matrix found at', patch_path)\n",
    "\n",
    "probe_w = None\n",
    "probe_layer = None\n",
    "if probe_w_paths:\n",
    "    w_path = probe_w_paths[-1]  # pick the latest by name sort\n",
    "    probe_layer = int(re.findall(r'probe_w_layer(\\d+)\\.npy', w_path)[0])\n",
    "    probe_w = np.load('results/' + w_path)\n",
    "    print('Loaded probe weights:', w_path, 'shape=', probe_w.shape)\n",
    "else:\n",
    "    print('No probe weights found in results/. Run 02_localization.ipynb first.')\n",
    "\n",
    "if STEER_LAYER is None and probe_layer is not None:\n",
    "    STEER_LAYER = probe_layer\n",
    "print('STEER_LAYER:', STEER_LAYER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task helpers (prompts, Bayes, model probability)\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bayesian_llm.bayes import two_generator_posterior_predictive\n",
    "from bayesian_llm.data import make_sequence, permute_sequence, set_seed\n",
    "\n",
    "set_seed(RNG_SEED)\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "X_tok = tl_model.to_single_token(' X')\n",
    "Y_tok = tl_model.to_single_token(' Y')\n",
    "\n",
    "\n",
    "def prompt_two_generators(sequence_tokens, *, control: str):\n",
    "    seq_str = ' '.join(sequence_tokens)\n",
    "    if control == 'base':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X.'\n",
    "    elif control == 'independent':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X. Draws are independent.'\n",
    "    elif control == 'order_irrelevant':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X. Draws are independent. Order does not matter.'\n",
    "    else:\n",
    "        raise ValueError(control)\n",
    "\n",
    "    return (\n",
    "        f\"{prefix} Sequence: {seq_str}. \"\n",
    "        'Predict the next output (X or Y):'\n",
    "    )\n",
    "\n",
    "\n",
    "def p_x_from_logits(logits):\n",
    "    p = torch.softmax(logits[0, -1, :].float(), dim=-1)\n",
    "    px = float(p[X_tok].item())\n",
    "    py = float(p[Y_tok].item())\n",
    "    return px / (px + py) if (px + py) > 0 else 0.5\n",
    "\n",
    "\n",
    "def p_x(prompt, *, fwd_hooks=None):\n",
    "    if fwd_hooks is None:\n",
    "        logits = tl_model(prompt)\n",
    "    else:\n",
    "        logits = tl_model.run_with_hooks(prompt, fwd_hooks=fwd_hooks)\n",
    "    return p_x_from_logits(logits)\n",
    "\n",
    "\n",
    "def evidence_sweep(*, control, fwd_hooks=None, n_total=N_TOTAL, n_perms=N_PERMUTATIONS):\n",
    "    rows = []\n",
    "    for n_x in range(n_total + 1):\n",
    "        base = make_sequence(n_x=n_x, n_total=n_total, x='X', y='Y')\n",
    "        true_p = two_generator_posterior_predictive(n_x=n_x, n_total=n_total)\n",
    "\n",
    "        preds = []\n",
    "        for _ in range(n_perms):\n",
    "            seq = permute_sequence(base, rng=rng)\n",
    "            prompt = prompt_two_generators(seq, control=control)\n",
    "            preds.append(p_x(prompt, fwd_hooks=fwd_hooks))\n",
    "\n",
    "        preds = np.asarray(preds)\n",
    "        rows.append({\n",
    "            'n_X': n_x,\n",
    "            'true_bayes': true_p,\n",
    "            'llm_mean': float(preds.mean()),\n",
    "            'llm_std': float(preds.std(ddof=1) if len(preds) > 1 else 0.0),\n",
    "            'order_drift': float(preds.max() - preds.min()),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['abs_error'] = (df.llm_mean - df.true_bayes).abs()\n",
    "    mae = float(df.abs_error.mean())\n",
    "    return df, mae\n",
    "\n",
    "\n",
    "def order_drift_fixed(*, seq_tokens, control, fwd_hooks=None, n_samples=50):\n",
    "    # Sample random permutations of a fixed multiset and measure drift.\n",
    "    preds = []\n",
    "    for _ in range(n_samples):\n",
    "        seq = permute_sequence(seq_tokens, rng=rng)\n",
    "        prompt = prompt_two_generators(seq, control=control)\n",
    "        preds.append(p_x(prompt, fwd_hooks=fwd_hooks))\n",
    "    preds = np.asarray(preds)\n",
    "    return float(preds.max() - preds.min()), preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: evidence sweep + order drift\n",
    "\n",
    "df_base, mae_base = evidence_sweep(control=CONTROL_TEMPLATE)\n",
    "print('Baseline MAE to Bayes:', mae_base)\n",
    "display(df_base)\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(df_base.n_X, df_base.true_bayes, color='black', linewidth=2, label='True Bayes')\n",
    "ax.errorbar(df_base.n_X, df_base.llm_mean, yerr=df_base.llm_std, marker='o', capsize=3, label='LLM')\n",
    "ax.set_title(f'Baseline sweep ({CONTROL_TEMPLATE})')\n",
    "ax.set_xlabel('n_X')\n",
    "ax.set_ylabel('P(next is X | {X,Y})')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Order drift diagnostic (fixed counts)\n",
    "seq_fixed = ['X','X','X','X','Y']\n",
    "drift, preds = order_drift_fixed(seq_tokens=seq_fixed, control=CONTROL_TEMPLATE, n_samples=100)\n",
    "print('Order drift for', seq_fixed, ':', drift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose candidate heads from head_patching_recovery.npy (if available)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "BAYES_HEADS = []\n",
    "ANTI_HEADS = []\n",
    "\n",
    "if head_recovery is not None:\n",
    "    flat = []\n",
    "    for layer in range(head_recovery.shape[0]):\n",
    "        for head in range(head_recovery.shape[1]):\n",
    "            flat.append((float(head_recovery[layer, head]), layer, head))\n",
    "    flat.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    BAYES_HEADS = [(layer, head) for _, layer, head in flat[:TOPK_BAYES]]\n",
    "    ANTI_HEADS = [(layer, head) for _, layer, head in flat[-TOPK_ANTI:]]\n",
    "\n",
    "print('BAYES_HEADS (top recovery):', BAYES_HEADS[:10], '... total', len(BAYES_HEADS))\n",
    "print('ANTI_HEADS (bottom recovery):', ANTI_HEADS[:10], '... total', len(ANTI_HEADS))\n",
    "\n",
    "# You can also override manually, e.g. BAYES_HEADS=[(20,3),(21,7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ablation hooks for a set of heads\n",
    "\n",
    "from transformer_lens.utils import get_act_name\n",
    "\n",
    "def ablation_hooks(heads_to_ablate):\n",
    "    # heads_to_ablate: list[(layer, head)]\n",
    "    layer_to_heads = defaultdict(list)\n",
    "    for layer, head in heads_to_ablate:\n",
    "        layer_to_heads[int(layer)].append(int(head))\n",
    "\n",
    "    hooks = []\n",
    "    for layer, heads in layer_to_heads.items():\n",
    "        hook_name = get_act_name('z', layer)\n",
    "        heads = sorted(set(heads))\n",
    "\n",
    "        def hook_fn(z, hook, heads=heads):\n",
    "            z[:, :, heads, :] = 0.0\n",
    "            return z\n",
    "\n",
    "        hooks.append((hook_name, hook_fn))\n",
    "    return hooks\n",
    "\n",
    "# Quick smoke test (no-op ablation)\n",
    "print('Hooks for 0 heads:', len(ablation_hooks([])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervention A: ablate ANTI_HEADS (hypothesis: improves Bayes calibration)\n",
    "\n",
    "if not ANTI_HEADS:\n",
    "    print('ANTI_HEADS empty; skipping')\n",
    "else:\n",
    "    hooks_anti = ablation_hooks(ANTI_HEADS)\n",
    "    df_anti, mae_anti = evidence_sweep(control=CONTROL_TEMPLATE, fwd_hooks=hooks_anti)\n",
    "\n",
    "    print('MAE baseline:', mae_base)\n",
    "    print('MAE ablate ANTI:', mae_anti)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    ax.plot(df_base.n_X, df_base.true_bayes, color='black', linewidth=2, label='True Bayes')\n",
    "    ax.plot(df_base.n_X, df_base.llm_mean, marker='o', label='Baseline')\n",
    "    ax.plot(df_anti.n_X, df_anti.llm_mean, marker='x', label='Ablate ANTI_HEADS')\n",
    "    ax.set_title('Ablating anti-recovery heads')\n",
    "    ax.set_xlabel('n_X')\n",
    "    ax.set_ylabel('P(next is X | {X,Y})')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    drift_base, _ = order_drift_fixed(seq_tokens=['X','X','X','X','Y'], control=CONTROL_TEMPLATE, n_samples=100)\n",
    "    drift_anti, _ = order_drift_fixed(seq_tokens=['X','X','X','X','Y'], control=CONTROL_TEMPLATE, fwd_hooks=hooks_anti, n_samples=100)\n",
    "    print('Order drift baseline:', drift_base)\n",
    "    print('Order drift ablate ANTI:', drift_anti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervention B: ablate BAYES_HEADS (hypothesis: harms Bayes tracking)\n",
    "\n",
    "if not BAYES_HEADS:\n",
    "    print('BAYES_HEADS empty; skipping')\n",
    "else:\n",
    "    hooks_bayes = ablation_hooks(BAYES_HEADS)\n",
    "    df_bayes, mae_bayes = evidence_sweep(control=CONTROL_TEMPLATE, fwd_hooks=hooks_bayes)\n",
    "\n",
    "    print('MAE baseline:', mae_base)\n",
    "    print('MAE ablate BAYES:', mae_bayes)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    ax.plot(df_base.n_X, df_base.true_bayes, color='black', linewidth=2, label='True Bayes')\n",
    "    ax.plot(df_base.n_X, df_base.llm_mean, marker='o', label='Baseline')\n",
    "    ax.plot(df_bayes.n_X, df_bayes.llm_mean, marker='x', label='Ablate BAYES_HEADS')\n",
    "    ax.set_title('Ablating high-recovery (candidate Bayes) heads')\n",
    "    ax.set_xlabel('n_X')\n",
    "    ax.set_ylabel('P(next is X | {X,Y})')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness: Distractors + Symbol Swap\n",
    "\n",
    "Here we test whether the inferred belief update survives nuisance changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distractor insertion\n",
    "\n",
    "DISTRACTOR_TEXT = ' The quick brown fox jumps over the lazy dog.'\n",
    "\n",
    "def prompt_with_distractor(sequence_tokens, *, control: str, distractor_repeats: int):\n",
    "    seq_str = ' '.join(sequence_tokens)\n",
    "    if control == 'base':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X.'\n",
    "    elif control == 'independent':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X. Draws are independent.'\n",
    "    elif control == 'order_irrelevant':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X. Draws are independent. Order does not matter.'\n",
    "    else:\n",
    "        raise ValueError(control)\n",
    "\n",
    "    distractor = DISTRACTOR_TEXT * int(distractor_repeats)\n",
    "    return (\n",
    "        f\"{prefix} Sequence: {seq_str}.\"\n",
    "        + distractor\n",
    "        + ' Predict the next output (X or Y):'\n",
    "    )\n",
    "\n",
    "\n",
    "def sweep_distractors(n_x, *, distractor_repeats_list, control, fwd_hooks=None):\n",
    "    base = make_sequence(n_x=n_x, n_total=N_TOTAL, x='X', y='Y')\n",
    "    base = permute_sequence(base, rng=rng)\n",
    "\n",
    "    rows = []\n",
    "    for r in distractor_repeats_list:\n",
    "        prompt = prompt_with_distractor(base, control=control, distractor_repeats=r)\n",
    "        rows.append({'repeats': r, 'p_x': p_x(prompt, fwd_hooks=fwd_hooks)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "repeats_list = [0, 1, 2, 4, 8]\n",
    "\n",
    "df_dist = sweep_distractors(n_x=8, distractor_repeats_list=repeats_list, control=CONTROL_TEMPLATE)\n",
    "display(df_dist)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(df_dist.repeats, df_dist.p_x, marker='o')\n",
    "ax.set_title('Effect of distractors on P(next=X)')\n",
    "ax.set_xlabel('Distractor repeats')\n",
    "ax.set_ylabel('P(next=X | {X,Y})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbol swap robustness (X/Y -> A/B)\n",
    "\n",
    "# This keeps the probabilistic structure but changes surface form.\n",
    "\n",
    "def prompt_two_generators_ab(sequence_tokens, *, control: str):\n",
    "    seq_str = ' '.join(sequence_tokens)\n",
    "    if control == 'base':\n",
    "        prefix = 'Two random generators. Generator A: 50% A. Generator B: 75% A.'\n",
    "    elif control == 'independent':\n",
    "        prefix = 'Two random generators. Generator A: 50% A. Generator B: 75% A. Draws are independent.'\n",
    "    elif control == 'order_irrelevant':\n",
    "        prefix = 'Two random generators. Generator A: 50% A. Generator B: 75% A. Draws are independent. Order does not matter.'\n",
    "    else:\n",
    "        raise ValueError(control)\n",
    "\n",
    "    return (\n",
    "        f\"{prefix} Sequence: {seq_str}. \"\n",
    "        'Predict the next output (A or B):'\n",
    "    )\n",
    "\n",
    "# Token ids in TLens for A/B\n",
    "A_tok = tl_model.to_single_token(' A')\n",
    "B_tok = tl_model.to_single_token(' B')\n",
    "\n",
    "def p_a(prompt, *, fwd_hooks=None):\n",
    "    logits = tl_model(prompt) if fwd_hooks is None else tl_model.run_with_hooks(prompt, fwd_hooks=fwd_hooks)\n",
    "    p = torch.softmax(logits[0, -1, :].float(), dim=-1)\n",
    "    pa = float(p[A_tok].item())\n",
    "    pb = float(p[B_tok].item())\n",
    "    return pa / (pa + pb) if (pa + pb) > 0 else 0.5\n",
    "\n",
    "seq = ['A','A','B','A','A']\n",
    "prompt_ab = prompt_two_generators_ab(seq, control=CONTROL_TEMPLATE)\n",
    "print(prompt_ab)\n",
    "print('P(next is A | {A,B}):', p_a(prompt_ab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering / Belief Injection\n",
    "\n",
    "We use a probe direction `w` (saved from `02_localization.ipynb`) to *add* a vector to the residual stream at a chosen layer.\n",
    "\n",
    "If `w` truly corresponds to belief in hypothesis B, increasing it should systematically increase `P(next=X)` on ambiguous evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a steering hook from probe direction (if available)\n",
    "\n",
    "from transformer_lens.utils import get_act_name\n",
    "\n",
    "def steering_hooks(*, layer: int, alpha: float, pos: int = -1):\n",
    "    if probe_w is None:\n",
    "        raise RuntimeError('No probe weights loaded. Run 02_localization.ipynb first.')\n",
    "\n",
    "    w = torch.tensor(probe_w, device=device, dtype=torch.float32)\n",
    "    w = w / (w.norm() + 1e-8)\n",
    "\n",
    "    hook_name = get_act_name('resid_post', int(layer))\n",
    "\n",
    "    def hook_fn(resid, hook):\n",
    "        resid[:, pos, :] = resid[:, pos, :] + alpha * w\n",
    "        return resid\n",
    "\n",
    "    return [(hook_name, hook_fn)]\n",
    "\n",
    "# Test on a moderately ambiguous sequence\n",
    "seq = ['X','X','X','Y','Y','Y','X','Y','X','Y']\n",
    "prompt = prompt_two_generators(seq, control=CONTROL_TEMPLATE)\n",
    "\n",
    "px0 = p_x(prompt)\n",
    "hooks = steering_hooks(layer=STEER_LAYER, alpha=STEER_ALPHA)\n",
    "px1 = p_x(prompt, fwd_hooks=hooks)\n",
    "print('Baseline P(X):', px0)\n",
    "print('Steered  P(X):', px1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steering dose-response curve\n",
    "\n",
    "alphas = [-10, -5, -2, -1, 0, 1, 2, 5, 10]\n",
    "rows = []\n",
    "for a in alphas:\n",
    "    hooks = steering_hooks(layer=STEER_LAYER, alpha=float(a))\n",
    "    rows.append({'alpha': a, 'p_x': p_x(prompt, fwd_hooks=hooks)})\n",
    "\n",
    "df_alpha = pd.DataFrame(rows)\n",
    "display(df_alpha)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(df_alpha.alpha, df_alpha.p_x, marker='o')\n",
    "ax.axhline(px0, color='gray', linestyle='--', alpha=0.6, label='baseline')\n",
    "ax.set_title('Belief steering dose-response')\n",
    "ax.set_xlabel('alpha (probe direction)')\n",
    "ax.set_ylabel('P(next=X | {X,Y})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intervention results (so you can compare across runs)\n",
    "\n",
    "import os\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "df_base.to_csv('results/causal_baseline_sweep.csv', index=False)\n",
    "print('Wrote results/causal_baseline_sweep.csv')\n",
    "\n",
    "if head_recovery is not None:\n",
    "    with open('results/selected_heads.txt', 'w') as f:\n",
    "        f.write('BAYES_HEADS\n",
    "')\n",
    "        for l,h in BAYES_HEADS:\n",
    "            f.write(f'{l},{h}\n",
    "')\n",
    "        f.write('\n",
    "ANTI_HEADS\n",
    "')\n",
    "        for l,h in ANTI_HEADS:\n",
    "            f.write(f'{l},{h}\n",
    "')\n",
    "    print('Wrote results/selected_heads.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
