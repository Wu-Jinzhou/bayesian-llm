{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Tests\n",
    "\n",
    "Quantify when next-token probabilities match the normative Bayesian posterior, and diagnose systematic deviations via competing cognitive models (tempered Bayes, recency/leak, frequency, Markov/induction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook path setup: make repo imports work regardless of where you run this from\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_candidates = [cwd, cwd.parent]\n",
    "repo_root = next((p for p in repo_candidates if (p / 'bayesian_llm').exists()), None)\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(f'Could not find repo root from cwd={cwd}.')\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print('Repo root:', repo_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports + configuration\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bayesian_llm.bayes import two_generator_posterior_predictive\n",
    "from bayesian_llm.data import make_sequence, permute_sequence, set_seed\n",
    "from bayesian_llm.llm import load_hf_causal_lm, normalized_next_token_prob\n",
    "from bayesian_llm.metrics import mae, pearson_r\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "MODEL_ID = 'meta-llama/Llama-3.1-8B'\n",
    "N_TOTAL = 10\n",
    "N_PERMUTATIONS = 10   # per (n_X) count; increase for tighter order-variance estimates\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "PRINT_EXAMPLES = True\n",
    "\n",
    "print('MODEL_ID:', MODEL_ID)\n",
    "print('N_TOTAL:', N_TOTAL)\n",
    "print('N_PERMUTATIONS:', N_PERMUTATIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (reuse this kernel for the whole notebook)\n",
    "import torch\n",
    "\n",
    "dtype = torch.float16\n",
    "loaded = load_hf_causal_lm(MODEL_ID, torch_dtype=dtype, device_map='auto')\n",
    "model, tokenizer = loaded.model, loaded.tokenizer\n",
    "\n",
    "print('Loaded:', MODEL_ID)\n",
    "print('dtype:', next(model.parameters()).dtype)\n",
    "print('device:', next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt builders (controls for order/independence wording)\n",
    "\n",
    "def prompt_two_generators(sequence_tokens, *, control: str = 'base'):\n",
    "    seq_str = ' '.join(sequence_tokens)\n",
    "    if control == 'base':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X.'\n",
    "    elif control == 'independent':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X. Draws are independent.'\n",
    "    elif control == 'order_irrelevant':\n",
    "        prefix = 'Two random generators. Generator A: 50% X. Generator B: 75% X. Draws are independent. Order does not matter.'\n",
    "    else:\n",
    "        raise ValueError(f'Unknown control={control}')\n",
    "\n",
    "    return (\n",
    "        f\"{prefix} Sequence: {seq_str}. \"\n",
    "        'Predict the next output (X or Y):'\n",
    "    )\n",
    "\n",
    "# Quick check\n",
    "example = prompt_two_generators(['X','X','Y','X','X'], control='order_irrelevant')\n",
    "print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evidence sweep with permutation (order) variance\n",
    "\n",
    "controls = ['base', 'independent', 'order_irrelevant']\n",
    "results = []\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "for control in controls:\n",
    "    for n_x in tqdm(range(N_TOTAL + 1), desc=f'sweep[{control}]'):\n",
    "        base_seq = make_sequence(n_x=n_x, n_total=N_TOTAL, x='X', y='Y')\n",
    "        true_p = two_generator_posterior_predictive(n_x=n_x, n_total=N_TOTAL)\n",
    "\n",
    "        preds = []\n",
    "        for _ in range(N_PERMUTATIONS):\n",
    "            seq = permute_sequence(base_seq, rng=rng)\n",
    "            prompt = prompt_two_generators(seq, control=control)\n",
    "            p_x = normalized_next_token_prob(\n",
    "                model, tokenizer, prompt,\n",
    "                a_variants=[' X', 'X', '\\nX'],\n",
    "                b_variants=[' Y', 'Y', '\\nY'],\n",
    "                temperature=TEMPERATURE,\n",
    "            )\n",
    "            preds.append(p_x)\n",
    "\n",
    "        preds = np.asarray(preds)\n",
    "        results.append({\n",
    "            'control': control,\n",
    "            'n_X': n_x,\n",
    "            'true_bayes': true_p,\n",
    "            'llm_mean': float(preds.mean()),\n",
    "            'llm_std': float(preds.std(ddof=1) if len(preds) > 1 else 0.0),\n",
    "            'llm_min': float(preds.min()),\n",
    "            'llm_max': float(preds.max()),\n",
    "            'drift_max_minus_min': float(preds.max() - preds.min()),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print('Rows:', len(df))\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Bayes curve vs LLM mean (with permutation std as error bars)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "for control in controls:\n",
    "    sub = df[df.control == control].sort_values('n_X')\n",
    "    ax.errorbar(\n",
    "        sub.n_X, sub.llm_mean, yerr=sub.llm_std,\n",
    "        label=f'LLM mean ({control})',\n",
    "        marker='o', capsize=3, alpha=0.8\n",
    "    )\n",
    "\n",
    "# Bayes is same across controls\n",
    "base = df[df.control == 'base'].sort_values('n_X')\n",
    "ax.plot(base.n_X, base.true_bayes, color='black', linewidth=2, label='True Bayes')\n",
    "\n",
    "ax.set_title('Evidence sweep (A=0.5 vs B=0.75)')\n",
    "ax.set_xlabel('n_X in N_TOTAL')\n",
    "ax.set_ylabel('P(next is X | {X,Y})')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Summary stats per control\n",
    "rows = []\n",
    "for control in controls:\n",
    "    sub = df[df.control == control]\n",
    "    rows.append({\n",
    "        'control': control,\n",
    "        'MAE_to_Bayes': mae(sub.true_bayes, sub.llm_mean),\n",
    "        'Pearson_r': pearson_r(sub.true_bayes, sub.llm_mean),\n",
    "        'Mean_order_drift': float(sub.drift_max_minus_min.mean()),\n",
    "        'Max_order_drift': float(sub.drift_max_minus_min.max()),\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(rows).sort_values('MAE_to_Bayes')\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic-prior control: compare X/Y vs H/T wording on the *same evidence*\n",
    "\n",
    "def prompt_coins(sequence_tokens):\n",
    "    seq_str = ' '.join(sequence_tokens)\n",
    "    return (\n",
    "        'Two coins. Type A is fair (50% heads). Type B is biased (75% heads). '\n",
    "        f'Sequence: {seq_str}. '\n",
    "        'Predict the next toss (H or T):'\n",
    "    )\n",
    "\n",
    "N_TOTAL_SEM = 10\n",
    "N_PERM_SEM = 5\n",
    "rng = np.random.default_rng(1)\n",
    "rows = []\n",
    "\n",
    "for n_h in tqdm(range(N_TOTAL_SEM + 1), desc='semantic_sweep'):\n",
    "    base_seq_ht = ['H'] * n_h + ['T'] * (N_TOTAL_SEM - n_h)\n",
    "    base_seq_xy = ['X'] * n_h + ['Y'] * (N_TOTAL_SEM - n_h)\n",
    "\n",
    "    true_p = two_generator_posterior_predictive(n_x=n_h, n_total=N_TOTAL_SEM)\n",
    "\n",
    "    preds_xy = []\n",
    "    preds_ht = []\n",
    "    for _ in range(N_PERM_SEM):\n",
    "        seq_ht = permute_sequence(base_seq_ht, rng=rng)\n",
    "        seq_xy = permute_sequence(base_seq_xy, rng=rng)\n",
    "\n",
    "        p_xy = normalized_next_token_prob(\n",
    "            model, tokenizer, prompt_two_generators(seq_xy, control='order_irrelevant'),\n",
    "            a_variants=[' X', 'X', '\\nX'], b_variants=[' Y', 'Y', '\\nY'],\n",
    "        )\n",
    "        p_ht = normalized_next_token_prob(\n",
    "            model, tokenizer, prompt_coins(seq_ht),\n",
    "            a_variants=[' H', 'H', '\\nH'], b_variants=[' T', 'T', '\\nT'],\n",
    "        )\n",
    "        preds_xy.append(p_xy)\n",
    "        preds_ht.append(p_ht)\n",
    "\n",
    "    rows.append({\n",
    "        'n_success': n_h,\n",
    "        'true_bayes': true_p,\n",
    "        'llm_xy_mean': float(np.mean(preds_xy)),\n",
    "        'llm_ht_mean': float(np.mean(preds_ht)),\n",
    "        'xy_minus_bayes': float(np.mean(preds_xy) - true_p),\n",
    "        'ht_minus_bayes': float(np.mean(preds_ht) - true_p),\n",
    "    })\n",
    "\n",
    "df_sem = pd.DataFrame(rows)\n",
    "display(df_sem.head())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(df_sem.n_success, df_sem.true_bayes, color='black', linewidth=2, label='True Bayes')\n",
    "ax.plot(df_sem.n_success, df_sem.llm_xy_mean, marker='o', label='LLM (X/Y, order_irrelevant)')\n",
    "ax.plot(df_sem.n_success, df_sem.llm_ht_mean, marker='x', label='LLM (H/T wording)')\n",
    "ax.set_title('Semantic control: abstract vs coin wording')\n",
    "ax.set_xlabel('n_success in N_TOTAL_SEM')\n",
    "ax.set_ylabel('P(next is success | {success,failure})')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print('MAE(X/Y):', mae(df_sem.true_bayes, df_sem.llm_xy_mean))\n",
    "print('MAE(H/T):', mae(df_sem.true_bayes, df_sem.llm_ht_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit simple cognitive models to the LLM curve (diagnosis, not claiming ground truth)\n",
    "\n",
    "# We fit against the 'order_irrelevant' curve to reduce order artifacts.\n",
    "sub = df[df.control == 'order_irrelevant'].sort_values('n_X')\n",
    "xs = sub.n_X.to_numpy()\n",
    "ys = sub.llm_mean.to_numpy()\n",
    "\n",
    "# Model 1: Tempered Bayes (likelihood exponent λ)\n",
    "# Posterior over A/B: loglik scaled by λ. Predictive then computed normally.\n",
    "def tempered_bayes_predict(n_x, n_total, lam):\n",
    "    # A: p=0.5, B: p=0.75\n",
    "    pA, pB = 0.5, 0.75\n",
    "    n_y = n_total - n_x\n",
    "    loglikA = n_x * math.log(pA) + n_y * math.log(1 - pA)\n",
    "    loglikB = n_x * math.log(pB) + n_y * math.log(1 - pB)\n",
    "    logpostA = math.log(0.5) + lam * loglikA\n",
    "    logpostB = math.log(0.5) + lam * loglikB\n",
    "    m = max(logpostA, logpostB)\n",
    "    postA = math.exp(logpostA - m)\n",
    "    postB = math.exp(logpostB - m)\n",
    "    Z = postA + postB\n",
    "    postA /= Z\n",
    "    postB /= Z\n",
    "    return pA * postA + pB * postB\n",
    "\n",
    "# Model 2: Linear frequency heuristic: a*(n_x/n_total)+b clipped\n",
    "def freq_predict(n_x, n_total, a, b):\n",
    "    p = a * (n_x / n_total) + b\n",
    "    return min(1.0, max(0.0, p))\n",
    "\n",
    "# Grid-search fit (kept simple / transparent)\n",
    "\n",
    "best = {}\n",
    "\n",
    "# Tempered Bayes\n",
    "lam_grid = np.linspace(0.1, 2.0, 40)\n",
    "errs = []\n",
    "for lam in lam_grid:\n",
    "    pred = np.array([tempered_bayes_predict(int(n), N_TOTAL, float(lam)) for n in xs])\n",
    "    errs.append(((pred - ys) ** 2).mean())\n",
    "best_lam = float(lam_grid[int(np.argmin(errs))])\n",
    "best['tempered_bayes'] = {'lam': best_lam, 'mse': float(min(errs))}\n",
    "\n",
    "# Frequency\n",
    "a_grid = np.linspace(0.0, 1.5, 61)\n",
    "b_grid = np.linspace(-0.25, 0.25, 81)\n",
    "best_mse = float('inf')\n",
    "best_ab = (None, None)\n",
    "for a in a_grid:\n",
    "    for b in b_grid:\n",
    "        pred = np.array([freq_predict(int(n), N_TOTAL, float(a), float(b)) for n in xs])\n",
    "        mse = float(((pred - ys) ** 2).mean())\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_ab = (float(a), float(b))\n",
    "best['frequency'] = {'a': best_ab[0], 'b': best_ab[1], 'mse': best_mse}\n",
    "\n",
    "print('Best fits:')\n",
    "for k,v in best.items():\n",
    "    print('-', k, v)\n",
    "\n",
    "# Plot fits\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(xs, sub.true_bayes.to_numpy(), color='black', linewidth=2, label='True Bayes')\n",
    "ax.plot(xs, ys, marker='o', label='LLM mean (order_irrelevant)')\n",
    "\n",
    "p_tb = np.array([tempered_bayes_predict(int(n), N_TOTAL, best_lam) for n in xs])\n",
    "ax.plot(xs, p_tb, linestyle='--', label=f'Tempered Bayes (λ={best_lam:.2f})')\n",
    "\n",
    "a,b = best_ab\n",
    "p_fr = np.array([freq_predict(int(n), N_TOTAL, a, b) for n in xs])\n",
    "ax.plot(xs, p_fr, linestyle='--', label=f'Frequency (a={a:.2f}, b={b:.2f})')\n",
    "\n",
    "ax.set_title('Diagnosing deviations: simple model fits')\n",
    "ax.set_xlabel('n_X')\n",
    "ax.set_ylabel('P(next is X | {X,Y})')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takeaways for next steps (printed so you can iterate quickly)\n",
    "\n",
    "best_control = summary.sort_values('MAE_to_Bayes').iloc[0].to_dict()\n",
    "print('Best prompt control (by MAE):', best_control)\n",
    "\n",
    "# Where does the LLM violate the known upper/lower bounds (0.5..0.75) for this task?\n",
    "sub = df[df.control == 'order_irrelevant'].sort_values('n_X')\n",
    "viol_upper = sub[sub.llm_mean > 0.75 + 1e-6][['n_X','llm_mean']].to_dict('records')\n",
    "viol_lower = sub[sub.llm_mean < 0.50 - 1e-6][['n_X','llm_mean']].to_dict('records')\n",
    "print('Upper-bound violations (should be <=0.75):', viol_upper[:10])\n",
    "print('Lower-bound violations (should be >=0.50):', viol_lower[:10])\n",
    "\n",
    "print('Next notebook suggestion: use logit-lens + activation patching to see which layers/heads push probabilities beyond Bayes.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
