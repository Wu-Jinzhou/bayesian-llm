{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Urn Task: Bayesian Cognition in LLMs (TransformerLens)\n\nThis notebook tests whether a model behaves like a Bayesian observer on a two-urn sequential inference task.\n\n**Key ideas**\n- Behavioral readout: infer belief from next-token logits for a forced-choice answer (A vs B).\n- Martingale drift: permutation invariance should hold under an i.i.d. model; deviations indicate order sensitivity.\n- Head ablation: identify and remove heads that drive drift.\n- Linear probes: decode Bayes posterior from internal states.\n\n**Roadmap**\n1) Single-episode demo (behavior) -> 2) Batch metrics\n3) Drift test -> 4) Head ablation scan -> 5) Ablated behavior\n6) Probes -> 7) Probe vs baseline vs ablated\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 1 - Install dependencies (Colab-safe)\n# If running locally and already installed, you can skip this cell.\n\n# !pip -q install transformer_lens circuitsvis einops pandas numpy matplotlib tqdm scikit-learn\n\n# Plotly renderer (CircuitVis)\ntry:\n    import plotly.io as pio\n    pio.renderers.default = 'notebook_connected'\nexcept Exception as e:\n    print('Plotly renderer not set:', e)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 2 - Imports + global seeds\nimport os\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom transformer_lens import HookedTransformer\nfrom transformer_lens.utils import get_act_name\n\nimport circuitsvis as cv\nfrom IPython.display import display\n\n# Reproducibility\nSEED = 0\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# Determinism (best-effort)\ntry:\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nexcept Exception:\n    pass\n\nprint('Seeds set to', SEED)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 3 - Global configuration (user-editable)\n\nMODEL_ID = 'meta-llama/Llama-3.2-3B'\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nDTYPE = torch.float16 if DEVICE == 'cuda' else torch.float32\n\n# Task config\nurn_A_pX = 0.75\nurn_B_pX = 0.25\nprior_P_A = 0.5\n\nseq_len_single = 20\nseq_len_batch = 20\nn_batch_episodes = 200\n\nprompt_template_version = 'v1'  # v1 | v2 | v3\n\n# Readout config (forced choice)\nANSWER_TOKEN_A = ' A'\nANSWER_TOKEN_B = ' B'\nANSWER_MEANS = {'A': 'X', 'B': 'Y'}\n\n# Drift config\ndrift_counts_total_N = 12\ndrift_counts_nX = 6\nn_permutations = 64\ndrift_metric = 'std'  # 'std' or 'max-min'\n\n# Head search config\nhead_search_scope = 'all'  # 'all' or 'layers_subset'\nlayers_subset = list(range(0, 4))  # used only if scope == 'layers_subset'\ntop_k_heads_to_report = 30\ntop_k_heads_to_ablate_default = 5\n\n# Ablation config\nheads_to_ablate = []  # fill after seeing ranking\nablate_mode = 'zero'  # 'zero' or 'scale'\nablate_scale = 0.0\n\n# Probe config\nprobe_target = 'posterior_logodds'  # 'posterior_logodds' or 'predictive_logodds'\nprobe_act_name = 'resid_post'  # 'resid_post' or 'resid_pre'\nprobe_position = 'answer_pos'  # last position\nprobe_train_size = 200\nprobe_test_size = 100\nprobe_epochs = 1  # unused for ridge; kept for API parity\nprobe_lr = 1e-3\nprobe_weight_decay = 0.0\nprobe_batch_size = 32\nprobe_type = 'ridge'  # 'ridge' or 'logistic' (ridge recommended)\n\n# Output\noutput_dir = 'results_urn_task'\nos.makedirs(output_dir, exist_ok=True)\n\nprint('MODEL_ID:', MODEL_ID)\nprint('DEVICE:', DEVICE, 'DTYPE:', DTYPE)\nprint('Task: A_pX=%.2f B_pX=%.2f prior=%.2f' % (urn_A_pX, urn_B_pX, prior_P_A))\nprint('Prompt template:', prompt_template_version)\nprint('Output dir:', output_dir)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 4 - Load model with TransformerLens\n\nmodel = HookedTransformer.from_pretrained(MODEL_ID, device=DEVICE, dtype=DTYPE)\nmodel.eval()\n\nprint('Loaded', MODEL_ID)\nprint('n_layers:', model.cfg.n_layers, 'n_heads:', model.cfg.n_heads, 'd_model:', model.cfg.d_model)\nprint('tokenizer:', type(model.tokenizer))\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 5 - Tokenization sanity checks (strict)\n\n\ndef assert_single_token(text):\n    ids = model.tokenizer.encode(text, add_special_tokens=False)\n    if len(ids) != 1:\n        raise RuntimeError(f'Tokenization error: {text!r} is not a single token (ids={ids}). Choose another label.')\n    return ids[0]\n\n# Check answer tokens\nA_id = assert_single_token(ANSWER_TOKEN_A)\nB_id = assert_single_token(ANSWER_TOKEN_B)\n\nprint('Chosen answer tokens:')\nprint('A token:', repr(ANSWER_TOKEN_A), 'id=', A_id, 'decoded=', model.tokenizer.decode([A_id]))\nprint('B token:', repr(ANSWER_TOKEN_B), 'id=', B_id, 'decoded=', model.tokenizer.decode([B_id]))\n\n# Also check plain A/B for info\nfor s in ['A','B']:\n    ids = model.tokenizer.encode(s, add_special_tokens=False)\n    print('Tokenization for', repr(s), '->', ids)\n\n# Observation symbol tokens (X/Y)\nobs_tokens = ['X','Y',' X',' Y']\nprint('Observation tokenization lengths:')\nfor s in obs_tokens:\n    ids = model.tokenizer.encode(s, add_special_tokens=False)\n    print(repr(s), 'len=', len(ids), 'ids=', ids)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 6 - Urn task math engine (ground truth Bayes)\n\n\ndef bayes_update_posterior(prior_P_A, pX_A, pX_B, obs_seq):\n    pA = prior_P_A\n    for obs in obs_seq:\n        if obs == 'X':\n            lik_A = pX_A\n            lik_B = pX_B\n        else:\n            lik_A = 1 - pX_A\n            lik_B = 1 - pX_B\n        unnorm_A = lik_A * pA\n        unnorm_B = lik_B * (1 - pA)\n        denom = unnorm_A + unnorm_B\n        pA = unnorm_A / denom if denom > 0 else 0.5\n    return pA\n\n\ndef bayes_predictive(posterior_P_A, pX_A, pX_B):\n    return posterior_P_A * pX_A + (1 - posterior_P_A) * pX_B\n\n\ndef logodds(p):\n    p = max(1e-9, min(1 - 1e-9, p))\n    return math.log(p / (1 - p))\n\n\ndef compute_truth_table(obs_seq, prior_P_A, pX_A, pX_B, laplace=0.0):\n    rows = []\n    pA = prior_P_A\n    nX = 0\n    for t, obs in enumerate(obs_seq, start=1):\n        if obs == 'X':\n            nX += 1\n        pA = bayes_update_posterior(prior_P_A, pX_A, pX_B, obs_seq[:t])\n        p_next_X = bayes_predictive(pA, pX_A, pX_B)\n        denom = t + 2 * laplace if laplace > 0 else t\n        p_hat = (nX + laplace) / denom if denom > 0 else 0.5\n        rows.append({\n            'step': t,\n            'obs': obs,\n            'posterior_P_A': pA,\n            'bayes_P_next_X': p_next_X,\n            'bayes_logodds': logodds(p_next_X),\n            'freq_Phat_X': p_hat,\n        })\n    return pd.DataFrame(rows)\n\nprint('Example truth table:')\nprint(compute_truth_table(['X','Y','X'], prior_P_A, urn_A_pX, urn_B_pX))\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 7 - Prompt design (behavioral, not calculation)\n\nTEMPLATES = {\n    'v1': (\n        'Two urns produce X/Y. Urn A makes X with probability {pA:.2f}. '\n        'Urn B makes X with probability {pB:.2f}. Prior P(A)={prior:.2f}. '\n        'Observed sequence: {seq}.\n'\n        'Next output is more likely:\n'\n        'A) X\n'\n        'B) Y\n'\n        'Answer:'\n    ),\n    'v2': (\n        'We have two generators. A: P(X)={pA:.2f}. B: P(X)={pB:.2f}. '\n        'Assume i.i.d. draws. Prior P(A)={prior:.2f}.\n'\n        'Sequence so far: {seq}.\n'\n        'Next is more likely?\n'\n        'A) X\n'\n        'B) Y\n'\n        'Answer:'\n    ),\n    'v3': (\n        'Urn A: {pA:.2f} of X. Urn B: {pB:.2f} of X. Prior(A)={prior:.2f}.\n'\n        'Evidence: {seq}.\n'\n        'Choose the more likely next symbol.\n'\n        'A) X\n'\n        'B) Y\n'\n        'Answer:'\n    ),\n}\n\n\ndef build_prompt(obs_seq, pA, pB, prior, template_id='v1'):\n    seq_str = ' '.join(obs_seq) if len(obs_seq) > 0 else '(none yet)'\n    return TEMPLATES[template_id].format(pA=pA, pB=pB, prior=prior, seq=seq_str)\n\nprint(build_prompt(['X','Y','X'], urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version))\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 8 - Extract model belief from logits (forced-choice readout)\n\n@torch.no_grad()\ndef get_ab_logprobs(prompt_str, token_id_A, token_id_B):\n    logits = model(prompt_str)\n    last = logits[0, -1, :]\n    logit_A = float(last[token_id_A].item())\n    logit_B = float(last[token_id_B].item())\n    probs = torch.softmax(last.float(), dim=-1)\n    pA = float(probs[token_id_A].item())\n    pB = float(probs[token_id_B].item())\n    denom = pA + pB\n    pA_norm = pA / denom if denom > 0 else 0.5\n    pB_norm = pB / denom if denom > 0 else 0.5\n    logodds_AB = math.log(pA_norm / pB_norm) if pA_norm > 0 and pB_norm > 0 else 0.0\n    return {\n        'logit_A': logit_A,\n        'logit_B': logit_B,\n        'pA_norm': pA_norm,\n        'pB_norm': pB_norm,\n        'logodds_AB': logodds_AB,\n    }\n\nprompt = build_prompt(['X','Y','X'], urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version)\nprint(get_ab_logprobs(prompt, A_id, B_id))\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 9 - Single-episode demo (dynamic sequence)\n\nhidden_urn = 'A' if random.random() < prior_P_A else 'B'\ntrue_pX = urn_A_pX if hidden_urn == 'A' else urn_B_pX\n\nobs_seq = ['X' if random.random() < true_pX else 'Y' for _ in range(seq_len_single)]\n\nrows = []\nfor t in range(1, seq_len_single + 1):\n    obs_prefix = obs_seq[:t]\n    prompt = build_prompt(obs_prefix, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version)\n    model_out = get_ab_logprobs(prompt, A_id, B_id)\n\n    truth = compute_truth_table(obs_prefix, prior_P_A, urn_A_pX, urn_B_pX)\n    bayes_p = float(truth.iloc[-1]['bayes_P_next_X'])\n    bayes_logodds = float(truth.iloc[-1]['bayes_logodds'])\n    freq_p = float(truth.iloc[-1]['freq_Phat_X'])\n\n    rows.append({\n        'step': t,\n        'obs_t': obs_prefix[-1],\n        'true_hidden_urn': hidden_urn,\n        'bayes_P_next_X': bayes_p,\n        'model_P_next_X': model_out['pA_norm'],\n        'freq_Phat_X': freq_p,\n        'bayes_logodds': bayes_logodds,\n        'model_logodds': model_out['logodds_AB'],\n    })\n\nsingle_df = pd.DataFrame(rows)\ndisplay(single_df.head())\n\nfig, ax = plt.subplots(figsize=(9, 4))\nax.plot(single_df.step, single_df.bayes_P_next_X, label='Bayes', marker='o')\nax.plot(single_df.step, single_df.model_P_next_X, label='Model', marker='x')\nax.plot(single_df.step, single_df.freq_Phat_X, label='Frequentist', linestyle='--')\n\nfor i, tok in enumerate(single_df.obs_t):\n    ax.annotate(tok, (single_df.step.iloc[i], 0.45), fontsize=10, ha='center',\n                color='black' if tok == 'X' else 'red')\n\nax.set_title('Single-episode tracking')\nax.set_xlabel('step')\nax.set_ylabel('P(next is X)')\nax.set_ylim(0.0, 1.0)\nax.grid(True, alpha=0.3)\nax.legend()\nplt.show()\n\nmae = float((single_df.model_P_next_X - single_df.bayes_P_next_X).abs().mean())\nbias = float((single_df.model_P_next_X - single_df.bayes_P_next_X).mean())\ncorr = float(np.corrcoef(single_df.model_P_next_X, single_df.bayes_P_next_X)[0, 1])\nprint('MAE:', mae, 'Bias:', bias, 'Corr:', corr)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 10 - Batch evaluation (no ablation)\n\nrows = []\nfor ep in tqdm(range(n_batch_episodes), desc='episodes'):\n    hidden_urn = 'A' if random.random() < prior_P_A else 'B'\n    true_pX = urn_A_pX if hidden_urn == 'A' else urn_B_pX\n    obs_seq = ['X' if random.random() < true_pX else 'Y' for _ in range(seq_len_batch)]\n\n    for t in range(1, seq_len_batch + 1):\n        obs_prefix = obs_seq[:t]\n        prompt = build_prompt(obs_prefix, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version)\n        model_out = get_ab_logprobs(prompt, A_id, B_id)\n\n        truth = compute_truth_table(obs_prefix, prior_P_A, urn_A_pX, urn_B_pX)\n        bayes_p = float(truth.iloc[-1]['bayes_P_next_X'])\n\n        rows.append({\n            'ep': ep,\n            't': t,\n            'bayes_P_next_X': bayes_p,\n            'model_P_next_X': model_out['pA_norm'],\n        })\n\nall_df = pd.DataFrame(rows)\n\nmae = float((all_df.model_P_next_X - all_df.bayes_P_next_X).abs().mean())\nbias = float((all_df.model_P_next_X - all_df.bayes_P_next_X).mean())\ncorr = float(np.corrcoef(all_df.model_P_next_X, all_df.bayes_P_next_X)[0, 1])\n\nsummary = pd.DataFrame([{'MAE': mae, 'Bias': bias, 'Corr': corr, 'n': len(all_df)}])\ndisplay(summary)\nsummary.to_csv(os.path.join(output_dir, 'batch_summary_baseline.csv'), index=False)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 11 - Martingale drift setup (permutation invariance test)\n\nbase_seq = ['X'] * drift_counts_nX + ['Y'] * (drift_counts_total_N - drift_counts_nX)\n\nperm_seqs = []\nperm_prompts = []\nrng = np.random.default_rng(SEED)\n\nfor _ in range(n_permutations):\n    seq = base_seq.copy()\n    rng.shuffle(seq)\n    perm_seqs.append(seq)\n    perm_prompts.append(build_prompt(seq, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version))\n\npreds = []\nfor prompt in tqdm(perm_prompts, desc='perm eval'):\n    out = get_ab_logprobs(prompt, A_id, B_id)\n    preds.append(out['pA_norm'])\n\nperm_df = pd.DataFrame({\n    'seq': [' '.join(s) for s in perm_seqs],\n    'model_P_next_X': preds,\n})\n\nif drift_metric == 'std':\n    drift_val = float(np.std(perm_df.model_P_next_X))\nelse:\n    drift_val = float(perm_df.model_P_next_X.max() - perm_df.model_P_next_X.min())\n\nposterior_P_A = bayes_update_posterior(prior_P_A, urn_A_pX, urn_B_pX, base_seq)\nbayes_pred = bayes_predictive(posterior_P_A, urn_A_pX, urn_B_pX)\n\nprint('Drift metric:', drift_metric, 'value:', drift_val)\nprint('Bayes predictive P(next=X):', bayes_pred)\n\nperm_df_sorted = perm_df.sort_values('model_P_next_X')\nprint('Lowest predictions:')\ndisplay(perm_df_sorted.head(3))\nprint('Highest predictions:')\ndisplay(perm_df_sorted.tail(3))\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 12 - Head ablation utilities (TransformerLens hooks)\n\nfrom collections import defaultdict\n\n\ndef make_head_ablation_hooks(head_indices, mode='zero', scale=0.0):\n    # head_indices: list of (layer, head) pairs\n    layer_to_heads = defaultdict(list)\n    for layer, head in head_indices:\n        layer_to_heads[int(layer)].append(int(head))\n\n    hooks = []\n    for layer, heads in layer_to_heads.items():\n        hook_name = f'blocks.{layer}.attn.hook_result'\n        heads = sorted(set(heads))\n\n        def hook_fn(act, hook, heads=heads, mode=mode, scale=scale):\n            # act: [batch, pos, head, d_head]\n            if mode == 'zero':\n                act[:, :, heads, :] = 0.0\n            elif mode == 'scale':\n                act[:, :, heads, :] = act[:, :, heads, :] * scale\n            else:\n                raise ValueError('Unknown mode')\n            return act\n\n        hooks.append((hook_name, hook_fn))\n    return hooks\n\n\n@torch.no_grad()\ndef get_pA_for_prompts_batch(prompts, hooks=None):\n    tokens = model.to_tokens(prompts, prepend_bos=True)\n    logits = model(tokens) if hooks is None else model.run_with_hooks(tokens, fwd_hooks=hooks)\n    last = logits[:, -1, :]\n    probs = torch.softmax(last.float(), dim=-1)\n\n    pA = probs[:, A_id]\n    pB = probs[:, B_id]\n    denom = pA + pB\n    pA_norm = pA / denom\n    return pA_norm.detach().cpu().numpy()\n\n\ndef run_model_with_optional_ablation(prompts, heads_to_ablate):\n    if heads_to_ablate is None or len(heads_to_ablate) == 0:\n        return get_pA_for_prompts_batch(prompts, hooks=None)\n    hooks = make_head_ablation_hooks(heads_to_ablate, mode=ablate_mode, scale=ablate_scale)\n    return get_pA_for_prompts_batch(prompts, hooks=hooks)\n\nprint('Ablation utilities ready')\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 13 - Identify drift-driving heads by ablation scan\n\nbaseline_preds = get_pA_for_prompts_batch(perm_prompts)\nif drift_metric == 'std':\n    drift_baseline = float(np.std(baseline_preds))\nelse:\n    drift_baseline = float(baseline_preds.max() - baseline_preds.min())\n\nprint('Baseline drift:', drift_baseline)\n\nif head_search_scope == 'all':\n    candidate_layers = list(range(model.cfg.n_layers))\nelse:\n    candidate_layers = list(layers_subset)\n\nrows = []\nfor layer in tqdm(candidate_layers, desc='layer scan'):\n    for head in range(model.cfg.n_heads):\n        hooks = make_head_ablation_hooks([(layer, head)], mode='zero')\n        preds = get_pA_for_prompts_batch(perm_prompts, hooks=hooks)\n        if drift_metric == 'std':\n            drift = float(np.std(preds))\n        else:\n            drift = float(preds.max() - preds.min())\n        rows.append({\n            'layer': layer,\n            'head': head,\n            'drift_baseline': drift_baseline,\n            'drift_ablated': drift,\n            'drift_reduction': drift_baseline - drift,\n            'mean_P_next_X_baseline': float(baseline_preds.mean()),\n            'mean_P_next_X_ablated': float(preds.mean()),\n        })\n\nhead_rank = pd.DataFrame(rows).sort_values('drift_reduction', ascending=False)\n\ndisplay(head_rank.head(top_k_heads_to_report))\n\nhead_rank.to_csv(os.path.join(output_dir, 'head_drift_ranking.csv'), index=False)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 14 - Choose heads to ablate (edit this cell after viewing ranking)\n\n# Option 1: manual selection\nheads_to_ablate = []\n\n# Option 2: auto-fill with top-K heads\n# heads_to_ablate = [(int(r.layer), int(r.head)) for r in head_rank.head(top_k_heads_to_ablate_default).itertuples()]\n\nprint('heads_to_ablate:', heads_to_ablate)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 15 - Single-episode demo with ablation (baseline vs ablated vs Bayes)\n\nrows = []\nfor t in range(1, seq_len_single + 1):\n    obs_prefix = obs_seq[:t]\n    prompt = build_prompt(obs_prefix, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version)\n\n    base = get_ab_logprobs(prompt, A_id, B_id)\n    abl = None\n    if heads_to_ablate:\n        hooks = make_head_ablation_hooks(heads_to_ablate, mode=ablate_mode, scale=ablate_scale)\n        logits = model.run_with_hooks(prompt, fwd_hooks=hooks)\n        last = logits[0, -1, :]\n        probs = torch.softmax(last.float(), dim=-1)\n        pA = float(probs[A_id].item())\n        pB = float(probs[B_id].item())\n        abl = pA / (pA + pB) if (pA + pB) > 0 else 0.5\n\n    truth = compute_truth_table(obs_prefix, prior_P_A, urn_A_pX, urn_B_pX)\n    bayes_p = float(truth.iloc[-1]['bayes_P_next_X'])\n    freq_p = float(truth.iloc[-1]['freq_Phat_X'])\n\n    rows.append({\n        'step': t,\n        'obs_t': obs_prefix[-1],\n        'bayes_P_next_X': bayes_p,\n        'model_P_next_X_base': base['pA_norm'],\n        'model_P_next_X_ablated': abl,\n        'freq_Phat_X': freq_p,\n    })\n\nabl_df = pd.DataFrame(rows)\ndisplay(abl_df.head())\n\nfig, ax = plt.subplots(figsize=(9, 4))\nax.plot(abl_df.step, abl_df.bayes_P_next_X, label='Bayes', marker='o')\nax.plot(abl_df.step, abl_df.model_P_next_X_base, label='Baseline', marker='x')\nif heads_to_ablate:\n    ax.plot(abl_df.step, abl_df.model_P_next_X_ablated, label='Ablated', marker='^')\nax.plot(abl_df.step, abl_df.freq_Phat_X, label='Frequentist', linestyle='--')\n\nfor i, tok in enumerate(abl_df.obs_t):\n    ax.annotate(tok, (abl_df.step.iloc[i], 0.45), fontsize=10, ha='center',\n                color='black' if tok == 'X' else 'red')\n\nax.set_title('Single-episode with ablation')\nax.set_xlabel('step')\nax.set_ylabel('P(next is X)')\nax.set_ylim(0.0, 1.0)\nax.grid(True, alpha=0.3)\nax.legend()\nplt.show()\n\nmae_base = float((abl_df.model_P_next_X_base - abl_df.bayes_P_next_X).abs().mean())\nbias_base = float((abl_df.model_P_next_X_base - abl_df.bayes_P_next_X).mean())\n\nprint('Baseline MAE:', mae_base, 'Bias:', bias_base)\nif heads_to_ablate:\n    mae_abl = float((abl_df.model_P_next_X_ablated - abl_df.bayes_P_next_X).abs().mean())\n    bias_abl = float((abl_df.model_P_next_X_ablated - abl_df.bayes_P_next_X).mean())\n    print('Ablated MAE:', mae_abl, 'Bias:', bias_abl)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 16 - Batch evaluation with ablation (baseline vs ablated)\n\nrows = []\nfor ep in tqdm(range(n_batch_episodes), desc='episodes'):\n    hidden_urn = 'A' if random.random() < prior_P_A else 'B'\n    true_pX = urn_A_pX if hidden_urn == 'A' else urn_B_pX\n    obs_seq_batch = ['X' if random.random() < true_pX else 'Y' for _ in range(seq_len_batch)]\n\n    for t in range(1, seq_len_batch + 1):\n        obs_prefix = obs_seq_batch[:t]\n        prompt = build_prompt(obs_prefix, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version)\n        base = get_ab_logprobs(prompt, A_id, B_id)['pA_norm']\n\n        abl = None\n        if heads_to_ablate:\n            hooks = make_head_ablation_hooks(heads_to_ablate, mode=ablate_mode, scale=ablate_scale)\n            logits = model.run_with_hooks(prompt, fwd_hooks=hooks)\n            last = logits[0, -1, :]\n            probs = torch.softmax(last.float(), dim=-1)\n            pA = float(probs[A_id].item())\n            pB = float(probs[B_id].item())\n            abl = pA / (pA + pB) if (pA + pB) > 0 else 0.5\n\n        truth = compute_truth_table(obs_prefix, prior_P_A, urn_A_pX, urn_B_pX)\n        bayes_p = float(truth.iloc[-1]['bayes_P_next_X'])\n\n        rows.append({\n            'bayes_P_next_X': bayes_p,\n            'model_P_next_X_base': base,\n            'model_P_next_X_ablated': abl,\n        })\n\nall_df2 = pd.DataFrame(rows)\n\nsummary_rows = []\nmae_base = float((all_df2.model_P_next_X_base - all_df2.bayes_P_next_X).abs().mean())\nbias_base = float((all_df2.model_P_next_X_base - all_df2.bayes_P_next_X).mean())\ncorr_base = float(np.corrcoef(all_df2.model_P_next_X_base, all_df2.bayes_P_next_X)[0, 1])\nsummary_rows.append({'model': 'baseline', 'MAE': mae_base, 'Bias': bias_base, 'Corr': corr_base})\n\nif heads_to_ablate:\n    mae_abl = float((all_df2.model_P_next_X_ablated - all_df2.bayes_P_next_X).abs().mean())\n    bias_abl = float((all_df2.model_P_next_X_ablated - all_df2.bayes_P_next_X).mean())\n    corr_abl = float(np.corrcoef(all_df2.model_P_next_X_ablated, all_df2.bayes_P_next_X)[0, 1])\n    summary_rows.append({'model': 'ablated', 'MAE': mae_abl, 'Bias': bias_abl, 'Corr': corr_abl})\n\nsummary = pd.DataFrame(summary_rows)\ndisplay(summary)\n\nbase_preds = get_pA_for_prompts_batch(perm_prompts)\nif drift_metric == 'std':\n    drift_base = float(np.std(base_preds))\nelse:\n    drift_base = float(base_preds.max() - base_preds.min())\n\nif heads_to_ablate:\n    ablated_preds = run_model_with_optional_ablation(perm_prompts, heads_to_ablate)\n    if drift_metric == 'std':\n        drift_abl = float(np.std(ablated_preds))\n    else:\n        drift_abl = float(ablated_preds.max() - ablated_preds.min())\n    print('Drift baseline:', drift_base, 'Drift ablated:', drift_abl, 'Reduction:', drift_base - drift_abl)\n\nsummary.to_csv(os.path.join(output_dir, 'batch_summary_ablation.csv'), index=False)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 17 - CircuitVis attention visualization (baseline and ablated)\n\nmin_seq = perm_df_sorted.head(1).iloc[0]['seq'].split(' ')\nmax_seq = perm_df_sorted.tail(1).iloc[0]['seq'].split(' ')\n\nfor label, seq in [('min', min_seq), ('max', max_seq)]:\n    prompt = build_prompt(seq, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version)\n    print('===', label, 'prompt ===')\n    print(prompt)\n    logits, cache = model.run_with_cache(prompt)\n    tokens = model.to_str_tokens(prompt)\n    for layer in range(model.cfg.n_layers):\n        patt = cache['pattern', layer][0].detach().cpu().numpy()\n        display(cv.attention.attention_patterns(tokens=tokens, attention=patt, title=f'{label}: layer {layer} (baseline)'))\n\n    if heads_to_ablate:\n        hooks = make_head_ablation_hooks(heads_to_ablate, mode=ablate_mode, scale=ablate_scale)\n        logits2, cache2 = model.run_with_cache(prompt, fwd_hooks=hooks)\n        for layer in range(model.cfg.n_layers):\n            patt2 = cache2['pattern', layer][0].detach().cpu().numpy()\n            display(cv.attention.attention_patterns(tokens=tokens, attention=patt2, title=f'{label}: layer {layer} (ablated)'))\n\nprint('Note: ablation changes outputs; attention patterns may be similar.')\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 18 - Build probe dataset (activations + labels)\n\nfrom collections import defaultdict\n\n\ndef generate_episode(pA, pB, prior, length):\n    hidden = 'A' if random.random() < prior else 'B'\n    pX = pA if hidden == 'A' else pB\n    obs = ['X' if random.random() < pX else 'Y' for _ in range(length)]\n    return hidden, obs\n\n\ndef get_target_logodds(obs_prefix):\n    pA = bayes_update_posterior(prior_P_A, urn_A_pX, urn_B_pX, obs_prefix)\n    if probe_target == 'posterior_logodds':\n        return logodds(pA)\n    p_next = bayes_predictive(pA, urn_A_pX, urn_B_pX)\n    return logodds(p_next)\n\n\ndef batch_by_length(prompts):\n    groups = defaultdict(list)\n    for i, p in enumerate(prompts):\n        length = len(model.tokenizer.encode(p, add_special_tokens=False)) + 1\n        groups[length].append(i)\n    return groups\n\ntrain_prompts = []\ntrain_labels = []\n\nfor _ in range(probe_train_size):\n    _, obs = generate_episode(urn_A_pX, urn_B_pX, prior_P_A, seq_len_batch)\n    for t in range(1, seq_len_batch + 1):\n        prefix = obs[:t]\n        train_prompts.append(build_prompt(prefix, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version))\n        train_labels.append(get_target_logodds(prefix))\n\ntrain_labels = np.array(train_labels, dtype=np.float32)\n\nX_by_layer = {layer: [] for layer in range(model.cfg.n_layers)}\nidx_groups = batch_by_length(train_prompts)\n\nfor length, idxs in tqdm(idx_groups.items(), desc='probe extract (train)'):\n    batch_prompts = [train_prompts[i] for i in idxs]\n    tokens = model.to_tokens(batch_prompts, prepend_bos=True)\n    logits, cache = model.run_with_cache(tokens)\n    for layer in range(model.cfg.n_layers):\n        act = cache[probe_act_name, layer][:, -1, :].detach().cpu().numpy()\n        X_by_layer[layer].append(act)\n\nfor layer in X_by_layer:\n    X_by_layer[layer] = np.concatenate(X_by_layer[layer], axis=0)\n\nprint('Probe dataset shape example:', X_by_layer[0].shape, 'labels:', train_labels.shape)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 19 - Train linear probes for all layers\n\nfrom sklearn.linear_model import Ridge\n\nresults = []\nprobe_weights = {}\n\nn_total = len(train_labels)\nidx = np.arange(n_total)\nnp.random.shuffle(idx)\nsplit = int(0.8 * n_total)\ntrain_idx, test_idx = idx[:split], idx[split:]\n\nfor layer in tqdm(range(model.cfg.n_layers), desc='probe train'):\n    X = X_by_layer[layer]\n    X_tr, X_te = X[train_idx], X[test_idx]\n    y_tr, y_te = train_labels[train_idx], train_labels[test_idx]\n\n    if probe_type == 'ridge':\n        reg = Ridge(alpha=1.0, fit_intercept=True)\n        reg.fit(X_tr, y_tr)\n        w = reg.coef_\n        b = float(reg.intercept_)\n    else:\n        raise ValueError('Only ridge implemented in this notebook')\n\n    pred = X_te @ w + b\n    pred_p = 1 / (1 + np.exp(-pred))\n    true_p = 1 / (1 + np.exp(-y_te))\n\n    mae = float(np.mean(np.abs(pred_p - true_p)))\n    mse = float(np.mean((pred - y_te) ** 2))\n    corr = float(np.corrcoef(pred_p, true_p)[0, 1])\n\n    results.append({'layer': layer, 'MAE': mae, 'MSE_logodds': mse, 'Corr': corr})\n    probe_weights[layer] = (w, b)\n\n    np.save(os.path.join(output_dir, f'probe_w_layer{layer}.npy'), w.astype(np.float32))\n    np.save(os.path.join(output_dir, f'probe_b_layer{layer}.npy'), np.array([b], dtype=np.float32))\n\nres_df = pd.DataFrame(results).sort_values('MAE')\ndisplay(res_df.head(10))\nres_df.to_csv(os.path.join(output_dir, 'probe_results.csv'), index=False)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 20 - Choose probe layer and run single-episode with probe overlay\n\nchosen_probe_layer = int(res_df.iloc[0]['layer'])\n\nw = np.load(os.path.join(output_dir, f'probe_w_layer{chosen_probe_layer}.npy'))\nb = float(np.load(os.path.join(output_dir, f'probe_b_layer{chosen_probe_layer}.npy'))[0])\n\nrows = []\nfor t in range(1, seq_len_single + 1):\n    obs_prefix = obs_seq[:t]\n    prompt = build_prompt(obs_prefix, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version)\n\n    base = get_ab_logprobs(prompt, A_id, B_id)['pA_norm']\n    abl = None\n    if heads_to_ablate:\n        hooks = make_head_ablation_hooks(heads_to_ablate, mode=ablate_mode, scale=ablate_scale)\n        logits = model.run_with_hooks(prompt, fwd_hooks=hooks)\n        last = logits[0, -1, :]\n        probs = torch.softmax(last.float(), dim=-1)\n        pA = float(probs[A_id].item())\n        pB = float(probs[B_id].item())\n        abl = pA / (pA + pB) if (pA + pB) > 0 else 0.5\n\n    tokens = model.to_tokens(prompt, prepend_bos=True)\n    logits, cache = model.run_with_cache(tokens)\n    act = cache[probe_act_name, chosen_probe_layer][0, -1, :].detach().cpu().numpy()\n    pred_logodds = act @ w + b\n    probe_p = 1 / (1 + np.exp(-pred_logodds))\n\n    truth = compute_truth_table(obs_prefix, prior_P_A, urn_A_pX, urn_B_pX)\n    bayes_p = float(truth.iloc[-1]['bayes_P_next_X'])\n\n    rows.append({\n        'step': t,\n        'obs_t': obs_prefix[-1],\n        'bayes_P_next_X': bayes_p,\n        'baseline_P_next_X': base,\n        'ablated_P_next_X': abl,\n        'probe_P_next_X': float(probe_p),\n    })\n\nprobe_df = pd.DataFrame(rows)\ndisplay(probe_df.head())\n\nfig, ax = plt.subplots(figsize=(9, 4))\nax.plot(probe_df.step, probe_df.bayes_P_next_X, label='Bayes', marker='o')\nax.plot(probe_df.step, probe_df.baseline_P_next_X, label='Baseline', marker='x')\nif heads_to_ablate:\n    ax.plot(probe_df.step, probe_df.ablated_P_next_X, label='Ablated', marker='^')\nax.plot(probe_df.step, probe_df.probe_P_next_X, label='Probe', marker='s')\n\nfor i, tok in enumerate(probe_df.obs_t):\n    ax.annotate(tok, (probe_df.step.iloc[i], 0.45), fontsize=10, ha='center',\n                color='black' if tok == 'X' else 'red')\n\nax.set_title('Single-episode with probe overlay')\nax.set_xlabel('step')\nax.set_ylabel('P(next is X)')\nax.set_ylim(0.0, 1.0)\nax.grid(True, alpha=0.3)\nax.legend()\nplt.show()\n\nfor name in ['baseline_P_next_X', 'probe_P_next_X']:\n    mae = float((probe_df[name] - probe_df.bayes_P_next_X).abs().mean())\n    bias = float((probe_df[name] - probe_df.bayes_P_next_X).mean())\n    print(name, 'MAE', mae, 'Bias', bias)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 21 - Batch evaluation including probe\n\nrows = []\nfor ep in tqdm(range(n_batch_episodes), desc='episodes'):\n    hidden_urn = 'A' if random.random() < prior_P_A else 'B'\n    true_pX = urn_A_pX if hidden_urn == 'A' else urn_B_pX\n    obs_seq_batch = ['X' if random.random() < true_pX else 'Y' for _ in range(seq_len_batch)]\n\n    for t in range(1, seq_len_batch + 1):\n        obs_prefix = obs_seq_batch[:t]\n        prompt = build_prompt(obs_prefix, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version)\n\n        base = get_ab_logprobs(prompt, A_id, B_id)['pA_norm']\n        abl = None\n        if heads_to_ablate:\n            hooks = make_head_ablation_hooks(heads_to_ablate, mode=ablate_mode, scale=ablate_scale)\n            logits = model.run_with_hooks(prompt, fwd_hooks=hooks)\n            last = logits[0, -1, :]\n            probs = torch.softmax(last.float(), dim=-1)\n            pA = float(probs[A_id].item())\n            pB = float(probs[B_id].item())\n            abl = pA / (pA + pB) if (pA + pB) > 0 else 0.5\n\n        tokens = model.to_tokens(prompt, prepend_bos=True)\n        logits, cache = model.run_with_cache(tokens)\n        act = cache[probe_act_name, chosen_probe_layer][0, -1, :].detach().cpu().numpy()\n        pred_logodds = act @ w + b\n        probe_p = 1 / (1 + np.exp(-pred_logodds))\n\n        truth = compute_truth_table(obs_prefix, prior_P_A, urn_A_pX, urn_B_pX)\n        bayes_p = float(truth.iloc[-1]['bayes_P_next_X'])\n\n        rows.append({\n            'bayes_P_next_X': bayes_p,\n            'baseline_P_next_X': base,\n            'ablated_P_next_X': abl,\n            'probe_P_next_X': float(probe_p),\n        })\n\nall_df3 = pd.DataFrame(rows)\n\nsummary_rows = []\nfor col in ['baseline_P_next_X', 'probe_P_next_X']:\n    mae = float((all_df3[col] - all_df3.bayes_P_next_X).abs().mean())\n    bias = float((all_df3[col] - all_df3.bayes_P_next_X).mean())\n    corr = float(np.corrcoef(all_df3[col], all_df3.bayes_P_next_X)[0, 1])\n    summary_rows.append({'model': col, 'MAE': mae, 'Bias': bias, 'Corr': corr})\n\nif heads_to_ablate:\n    col = 'ablated_P_next_X'\n    mae = float((all_df3[col] - all_df3.bayes_P_next_X).abs().mean())\n    bias = float((all_df3[col] - all_df3.bayes_P_next_X).mean())\n    corr = float(np.corrcoef(all_df3[col], all_df3.bayes_P_next_X)[0, 1])\n    summary_rows.append({'model': col, 'MAE': mae, 'Bias': bias, 'Corr': corr})\n\nsummary = pd.DataFrame(summary_rows)\ndisplay(summary)\nsummary.to_csv(os.path.join(output_dir, 'batch_summary_with_probe.csv'), index=False)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 22 - Optional: belief editing and belief swapping (causal validation)\n\nlayer_to_edit = chosen_probe_layer\nedit_lambda = 5.0\n\nprobe_dir = w / (np.linalg.norm(w) + 1e-8)\nprobe_dir_t = torch.tensor(probe_dir, device=DEVICE, dtype=torch.float32)\n\n\ndef belief_edit_hook(act, hook, direction, scale):\n    act[:, -1, :] = act[:, -1, :] + scale * direction\n    return act\n\nexample_seq = ['X','Y','X','Y','X','Y']\nexample_prompt = build_prompt(example_seq, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version)\n\nbase = get_ab_logprobs(example_prompt, A_id, B_id)['pA_norm']\n\nif probe_act_name == 'resid_post':\n    hook_name = f'blocks.{layer_to_edit}.hook_resid_post'\nelif probe_act_name == 'resid_pre':\n    hook_name = f'blocks.{layer_to_edit}.hook_resid_pre'\nelse:\n    raise ValueError('Unsupported probe_act_name for editing')\n\nlogits = model.run_with_hooks(example_prompt, fwd_hooks=[(hook_name, lambda act, hook: belief_edit_hook(act, hook, probe_dir_t, edit_lambda))])\nlast = logits[0, -1, :]\nprobs = torch.softmax(last.float(), dim=-1)\nedit_pA = float(probs[A_id].item())\nedit_pB = float(probs[B_id].item())\nedit = edit_pA / (edit_pA + edit_pB) if (edit_pA + edit_pB) > 0 else 0.5\n\nprint('Baseline P(X):', base)\nprint('Edited   P(X):', edit, '(lambda=', edit_lambda, ')')\nprint('Belief swapping is left as an exercise: patch the probe component between two runs.')\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Cell 23 - Multi-model runner (lightweight)\n\nMODEL_LIST = [\n    'meta-llama/Llama-3.2-1B',\n    'meta-llama/Llama-3.2-1B-Instruct',\n    'meta-llama/Llama-3.2-3B',\n    'meta-llama/Llama-3.2-3B-Instruct',\n    'meta-llama/Llama-3.1-8B',\n    'meta-llama/Llama-3.1-8B-Instruct',\n]\n\nRUN_MULTI_MODEL = False  # set True to execute\n\nif not RUN_MULTI_MODEL:\n    print('Set RUN_MULTI_MODEL=True to run across models.')\nelse:\n    rows = []\n    for mid in MODEL_LIST:\n        print('Loading', mid)\n        m = HookedTransformer.from_pretrained(mid, device=DEVICE, dtype=DTYPE)\n        m.eval()\n\n        # Token check per model\n        def assert_single_token_local(text):\n            ids = m.tokenizer.encode(text, add_special_tokens=False)\n            if len(ids) != 1:\n                raise RuntimeError(f'Model {mid}: token {text!r} is not single token (ids={ids})')\n            return ids[0]\n\n        A_id_local = assert_single_token_local(ANSWER_TOKEN_A)\n        B_id_local = assert_single_token_local(ANSWER_TOKEN_B)\n\n        tmp_preds = []\n        for _ in range(20):\n            hidden = 'A' if random.random() < prior_P_A else 'B'\n            pX = urn_A_pX if hidden == 'A' else urn_B_pX\n            obs = ['X' if random.random() < pX else 'Y' for _ in range(10)]\n            prompt = build_prompt(obs, urn_A_pX, urn_B_pX, prior_P_A, prompt_template_version)\n            logits = m(prompt)\n            last = logits[0, -1, :]\n            pA = float(torch.softmax(last.float(), dim=-1)[A_id_local].item())\n            pB = float(torch.softmax(last.float(), dim=-1)[B_id_local].item())\n            tmp_preds.append(pA / (pA + pB))\n        drift = float(np.std(tmp_preds))\n        rows.append({'model': mid, 'drift_proxy_std': drift})\n\n    multi_df = pd.DataFrame(rows)\n    display(multi_df)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}